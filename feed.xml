<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://www.siddarthasokan.com/feed.xml" rel="self" type="application/atom+xml"/><link href="https://www.siddarthasokan.com/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-17T16:05:48+00:00</updated><id>https://www.siddarthasokan.com/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Thesis Chapter 1.3: Flow-based Models and Diffusion</title><link href="https://www.siddarthasokan.com/thesis-chapters/2023/05/10/thesis-chapter-1p3-FlowsDiffusion.html" rel="alternate" type="text/html" title="Thesis Chapter 1.3: Flow-based Models and Diffusion"/><published>2023-05-10T02:00:00+00:00</published><updated>2023-05-10T02:00:00+00:00</updated><id>https://www.siddarthasokan.com/thesis-chapters/2023/05/10/thesis-chapter-1p3-FlowsDiffusion</id><content type="html" xml:base="https://www.siddarthasokan.com/thesis-chapters/2023/05/10/thesis-chapter-1p3-FlowsDiffusion.html"><![CDATA[<blockquote> <p><em>What I cannot create, I do not understand.</em><br/> — Richard P. Feynman</p> </blockquote> <h2 id="flow-based-models">Flow-based Models</h2> <p>Flow-based models such as normalizing flows (NFs) [1] leverage the <em>change-of-variables</em> formula to learn a transformation from a parametric prior distribution to the target. Unlike in GANs and AEs, in NF models, the input to model \(\mathbf{z}\) is assumed to be of the same dimensionality as the output, <em>i.e.</em>, \(\mathbf{z}\in\mathbb{R}^{n}\). Normalizing flows model a forward process as the gradual <em>corruption</em> of a target distribution \(p_d\) to the noise distribution \(p_z\). The forward process is modeled as a series of <em>invertible</em> transformations \(f_i\), such that the composite function yields the desired <em>push-forward</em> distribution:</p> \[\mathbf{z} = \left( f_N \circ f_{N-1} \circ \cdots \circ f_2 \circ f_1\right) (\mathbf{x}) = f(\mathbf{x});~\mathbf{x}\sim p_d.\] <p>Then, by the <em>change-of-variables</em> formula, we have</p> \[p_z\left(\mathbf{z}\right) = p_d\left(f^{-1}(\mathbf{z})\right) \left\vert \mathrm{det}\, \mathbf{J}_{f^{-1}}(\mathbf{z})\right\vert = p_d\left(f^{-1}(\mathbf{z})\right) \left\vert \mathrm{det}\, \mathbf{J}_{f}\left(f^{-1}(\mathbf{z})\right)\right\vert^{-1},\] <p>where \(\left\vert \mathrm{det}\, \mathbf{J}_{f}(\cdot)\right\vert\) denotes the determinant of the Jacobian of \(f\). Consider a vector \(\mathbf{x} = [x_1, x_2,\,\ldots\,,x_n]^{\mathsf{T}} \in \mathbb{R}^n\) and the function \(f:\mathbb{R}^n \rightarrow \mathbb{R}^n\), <em>i.e.</em>, \(f(\mathbf{x}) = [f^1(\mathbf{x}), f^2(\mathbf{x}),\ldots,f^n(\mathbf{x})]\). The notation \(\nabla_{\mathbf{x}} f(\mathbf{x})\) represents the gradient matrix associated with the generator, with entries consisting of the partial derivatives of the entries of \(f\) with respect to the entries of \(\mathbf{x}\):</p> \[\nabla_{\mathbf{x}} f(\mathbf{x}) = \left[\begin{matrix} \frac{\partial f^1}{\partial x_1} &amp; \frac{\partial f^2}{\partial x_1} &amp; \ldots &amp; \ldots &amp;\frac{\partial f^n}{\partial x_1} \\[3pt] \frac{\partial f^1}{\partial x_2} &amp; \frac{\partial f^2}{\partial x_2} &amp; \ldots &amp; \ldots &amp;\frac{\partial f^n}{\partial x_2} \\[3pt] \vdots&amp;\vdots&amp;\ddots&amp;\ddots&amp;\vdots\\ \frac{\partial f^1}{\partial x_n} &amp; \frac{\partial f^2}{\partial x_n} &amp; \ldots &amp; \ldots &amp;\frac{\partial f^n}{\partial x_n} \end{matrix} \right].\] <p>The Jacobian \(\mathbf{J}\) can be thought of as <em>measuring</em> the transformation that the function imposes locally at the point of evaluation, and is defined to be the transpose of the gradient, <em>i.e.,</em> \(\nabla_{\mathbf{z}}f(\mathbf{z}) = \mathbf{J}_f^{\mathsf{T}}(\mathbf{z})\). The generative model is then defined as the <em>reverse</em> process, given by:</p> \[p_d(\mathbf{x}) = p_z\left(f^{-1}(\mathbf{x})\right) \left\vert \mathrm{det}\, \mathbf{J}_{f}(\mathbf{x})\right\vert = p_z\left(f^{-1}(\mathbf{x})\right) \prod_{i=1}^{N} \left\vert \mathrm{det}\, \mathbf{J}_{f_i}(\mathbf{z}_{i-1})\right\vert,\] <p>where \(\mathbf{z}_i = f_i(\mathbf{z}_{i-1})\) with \(\mathbf{z}_0 = \mathbf{z}\) and \(\mathbf{z}_N = \mathbf{x}\). An in-depth analysis of normalizing flows is presented by recent comprehensive surveys [2]. The \(f_i\) network architecture is constrained to facilitate easy computation of the Jacobian. Recent approaches design flows based on autoregressive models [3,4,5,6,7], or architectures motivated by the Sobolev GAN loss [9,10]. Flows have also been explored in the context of improving the quality of the noise input in GANs [8,11,12].</p> <h2 id="score-based-diffusion-models">Score-based Diffusion Models</h2> <p>Diffusion models are a recent class of generative models that implement discretized Langevin flow [13], and can be viewed as a random walk in a high-dimensional space. The random walks correspond to a Markov chain, whose stationary distribution converges to the desired distribution. As in the case of flow-based approaches, a forward process is assumed, wherein the data is <em>corrupted</em> with noise, while the generative model is an approximation of the reverse process. Langevin Monte Carlo (LMC) is the canonical algorithm for sampling from a given distribution in this framework, where we have access to the <em>score function</em>, \(\nabla_{\mathbf{x}}\ln p_d(\mathbf{x})\), which is the gradient of the logarithm of the target distribution. LMC is an instance of Markov chain Monte Carlo (MCMC) [16], and is a discretization of the Langevin diffusion stochastic differential equation given by</p> \[d\mathbf{x}(t) = -\nabla_{\mathbf{x}}f(\mathbf{x}(t))\,dt + \sqrt{2}\,d\mathbf{B}(t),\] <p>where \(\mathbf{B}(t)\) is the standard Brownian motion. The associated discretized update is given by</p> \[\mathbf{x}_{t+1} = \mathbf{x}_{t} - \epsilon \left. \nabla_{\mathbf{x}}f(\mathbf{x}) \right|_{\mathbf{x}=\mathbf{x}_t} + \sqrt{2\epsilon}\,\mathbf{z}_t,\] <p>where \(\mathbf{z}_{t}\) is an instance of the noise distribution drawn at time instant \(t\) and is independent of the sample \(\mathbf{x}_{t}\) generated at the corresponding time instant. As in normalizing flows, we have \(\mathbf{z}\in\mathbb{R}^{n}\). The evolution is initialized with parametric noise, <em>i.e.</em>, \(\mathbf{x}_0 \sim p_z\). The choice of the mapping function \(f\) gives rise to various diffusion models. For example, the popular family of score-based models [14,15] consider \(f(\mathbf{x}) = \nabla_{\mathbf{x}}\ln p_d(\mathbf{x})\), or in inverse-heat diffusion models [24], we have a frequency-domain transformation corresponding to Gaussian deblurring. In this thesis, we primarily focus on score-based diffusion, and their relation to GAN optimization.</p> <p>Existing score-based approaches train a neural network \(s_{\theta}(\mathbf{x})\) to approximate the score, by means of a score-matching loss [17], originally considered in the context of independent component analysis:</p> \[\mathcal{L}(\theta)=\frac{1}{2} \,\mathbb{E}_\sim p_d} \left[ \left\Vert s_{\theta}(\mathbf{x}) - \nabla_{\mathbf{x}}\ln p_d(\mathbf{x}) \right\Vert \right\Vert_2^2 \right].\] <p>The output of the trained network is used to generate samples through the annealed Langevin dynamics in noise-conditioned score networks (NCSN) [14]. However, a major limitation is that the predicted score is <em>weak</em> in regions far away from the target distribution, which is typically the case at the start of the Markov chain, around \(\mathbf{x}_0\sim p_z\). Various approaches such as noise scaling [14], sliced SM [18], and denoising SM [13,15] have been proposed to improve the strength of the gradients. Works considering improved discretization of the underlying differential equation [15,20,21,22] have been developed to accelerate the sampling process. Recently, denoising diffusion GANs (DDGANs) [23] were introduced, wherein a GAN is trained to model the diffusion process, with the generator and discriminator networks conditioned on the time index.</p> <h3 id="navigation">Navigation</h3> <ul> <li><strong>Previous:</strong> <a href="/thesis-chapters/2023/05/10/thesis-chapter-1p2-IntroGANs.html">Chapter 1.2: Generative Adversarial Networks</a></li> <li><strong>Next:</strong> <a href="/blog/2023/thesis-chapter-1p4-VariationalCalculus/">Chapter 1.4: An Introduction to Variational Calculus</a></li> <li><strong>Back to:</strong> <a href="/projects/1_thesis/">Thesis Project</a></li> </ul> <hr/> <h2 id="references">References</h2> <ol> <li> <p><strong>Rezende, D., &amp; Mohamed, S. (2015).</strong> Variational inference with normalizing flows. In <em>International Conference on Machine Learning (ICML)</em> (pp. 1530-1538).</p> </li> <li> <p><strong>Papamakarios, G., Nalisnick, E., Rezende, D. J., Mohamed, S., &amp; Lakshminarayanan, B. (2021).</strong> Normalizing flows for probabilistic modeling and inference. <em>Journal of Machine Learning Research</em>, 22(57), 1-64.</p> </li> <li> <p><strong>Dinh, L., Krueger, D., &amp; Bengio, Y. (2015).</strong> NICE: Non-linear independent components estimation. In <em>International Conference on Learning Representations (ICLR) Workshop</em>.</p> </li> <li> <p><strong>Dinh, L., Sohl-Dickstein, J., &amp; Bengio, S. (2017).</strong> Density estimation using Real NVP. In <em>International Conference on Learning Representations (ICLR)</em>.</p> </li> <li> <p><strong>Kingma, D. P., &amp; Dhariwal, P. (2018).</strong> Glow: Generative flow using invertible 1×1 convolutions. In <em>Advances in Neural Information Processing Systems 31 (NeurIPS 2018)</em> (pp. 10215-10224).</p> </li> <li> <p><strong>Kingma, D. P., Salimans, T., Jozefowicz, R., Chen, X., Sutskever, I., &amp; Welling, M. (2016).</strong> Improved variational inference with inverse autoregressive flow. In <em>Advances in Neural Information Processing Systems 29 (NIPS 2016)</em> (pp. 4743-4751).</p> </li> <li> <p><strong>Papamakarios, G., Pavlakou, T., &amp; Murray, I. (2017).</strong> Masked autoregressive flow for density estimation. In <em>Advances in Neural Information Processing Systems 30 (NIPS 2017)</em> (pp. 2338-2347).</p> </li> <li> <p><strong>Chen, X., Kingma, D. P., Salimans, T., Duan, Y., Dhariwal, P., Schulman, J., Sutskever, I., &amp; Abbeel, P. (2018).</strong> Variational lossy autoencoder. In <em>International Conference on Learning Representations (ICLR)</em>.</p> </li> <li> <p><strong>Mroueh, Y., Sercu, T., &amp; Goel, V. (2019).</strong> Sobolev descent. In <em>International Conference on Artificial Intelligence and Statistics (AISTATS)</em> (pp. 2976-2984).</p> </li> <li> <p><strong>Gong, C., Wang, D., &amp; Liu, Q. (2020).</strong> Unbiased Sobolev descent. In <em>International Conference on Machine Learning (ICML)</em> (pp. 3558-3567).</p> </li> <li> <p><strong>Kumar, M., Weissenborn, D., &amp; Kalchbrenner, N. (2021).</strong> Regularizing normalizing flows with Kallenberg-Leibler divergence. In <em>Conference on Uncertainty in Artificial Intelligence (UAI)</em>.</p> </li> <li> <p><strong>Dinh, L., Sohl-Dickstein, J., Pascanu, R., &amp; Larochelle, H. (2021).</strong> A RAD approach to deep mixture models. In <em>International Conference on Learning Representations (ICLR)</em>.</p> </li> <li> <p><strong>Ho, J., Jain, A., &amp; Abbeel, P. (2020).</strong> Denoising diffusion probabilistic models. In <em>Advances in Neural Information Processing Systems 33 (NeurIPS 2020)</em> (pp. 6840-6851).</p> </li> <li> <p><strong>Song, Y., &amp; Ermon, S. (2019).</strong> Generative modeling by estimating gradients of the data distribution. In <em>Advances in Neural Information Processing Systems 32 (NeurIPS 2019)</em> (pp. 11895-11907).</p> </li> <li> <p><strong>Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., &amp; Poole, B. (2021).</strong> Score-based generative modeling through stochastic differential equations. In <em>International Conference on Learning Representations (ICLR)</em>.</p> </li> <li> <p><strong>Sinclair, A. (1992).</strong> Improved bounds for mixing rates of Markov chains and multicommodity flow. <em>Combinatorics, Probability and Computing</em>, 1(4), 351-370.</p> </li> <li> <p><strong>Hyvärinen, A. (2005).</strong> Estimation of non-normalized statistical models by score matching. <em>Journal of Machine Learning Research</em>, 6, 695-709.</p> </li> <li> <p><strong>Song, Y., &amp; Ermon, S. (2020).</strong> Improved techniques for training score-based generative models. In <em>Advances in Neural Information Processing Systems 33 (NeurIPS 2020)</em> (pp. 12438-12448).</p> </li> <li> <p><strong>Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., &amp; Poole, B. (2021).</strong> Score-based generative modeling through stochastic differential equations. In <em>International Conference on Learning Representations (ICLR)</em>.</p> </li> <li> <p><strong>Song, Y., Durkan, C., Murray, I., &amp; Ermon, S. (2020).</strong> Maximum likelihood training of score-based diffusion models. In <em>Advances in Neural Information Processing Systems 33 (NeurIPS 2020)</em> (pp. 1415-1428).</p> </li> <li> <p><strong>Jolicoeur-Martineau, A., Li, K., Piché-Taillefer, R., Kachman, T., &amp; Mitliagkas, I. (2021).</strong> Gotta go fast when generating data with score-based models. <em>arXiv preprint arXiv:2105.14080</em>.</p> </li> <li> <p><strong>Karras, T., Aittala, M., Aila, T., &amp; Laine, S. (2022).</strong> Elucidating the design space of diffusion-based generative models. In <em>Advances in Neural Information Processing Systems 35 (NeurIPS 2022)</em> (pp. 26565-26577).</p> </li> <li> <p><strong>Xiao, Z., Kreis, K., &amp; Vahdat, A. (2022).</strong> Tackling the generative learning trilemma with denoising diffusion GANs. In <em>International Conference on Learning Representations (ICLR)</em>.</p> </li> <li> <p><strong>Bansal, A., Borgnia, E., Chu, H.-M., Li, J., Kazemi, H., Huang, F., Goldblum, M., Geiping, J., &amp; Goldstein, T. (2023).</strong> Cold diffusion: Inverting arbitrary image transforms without noise. In <em>Advances in Neural Information Processing Systems 36 (NeurIPS 2023)</em>.</p> </li> </ol>]]></content><author><name></name></author><category term="thesis-chapters"/><category term="thesis"/><category term="flows"/><category term="diffusion"/><category term="machine-learning"/><category term="generative-models"/><summary type="html"><![CDATA[An introduction to normalizing flows and score-based diffusion models]]></summary></entry><entry><title type="html">Thesis Chapter 1.2: Introduction to Generative Adversarial Networks</title><link href="https://www.siddarthasokan.com/thesis-chapters/2023/05/10/thesis-chapter-1p2-introGANs.html" rel="alternate" type="text/html" title="Thesis Chapter 1.2: Introduction to Generative Adversarial Networks"/><published>2023-05-10T01:00:00+00:00</published><updated>2023-05-10T01:00:00+00:00</updated><id>https://www.siddarthasokan.com/thesis-chapters/2023/05/10/thesis-chapter-1p2-introGANs</id><content type="html" xml:base="https://www.siddarthasokan.com/thesis-chapters/2023/05/10/thesis-chapter-1p2-introGANs.html"><![CDATA[<blockquote> <p><em>What I cannot create, I do not understand.</em><br/> — Richard P. Feynman</p> </blockquote> <h2 id="generative-adversarial-networks-gans">Generative Adversarial Networks (GANs)</h2> <p>The original GAN formulation presented by Goodfellow et al. (2014) [1], (hereafter referred to as the Standard GAN, or SGAN) is a <em>min-max</em> game between two players — a generator (\(G\)), and a discriminator (\(D\)), both of which are typically implemented as neural networks. The role of the generator, akin to the decoder in AEs and VAEs, is to create fake samples that mimic the ones coming from the training data distribution. However, the generated samples are not trained with a one-to-one correspondence with the real/target data samples. Instead, the generator is trained to create samples that appear realistic to a <em>critic</em>, which is the discriminator network \(D\) tasked with telling apart the real samples from the fake ones. The optimal \(G\) is the one that <em>outsmarts</em> \(D\) into confusing the fake samples for real.</p> <p>We briefly introduce the GAN optimization framework here. An in-depth discussion of various GAN losses and their regularization schemes is presented in Section 1.3, while a comprehensive study of GAN algorithms and training practices for image generation, video synthesis, and neural rendering is provided in recent surveys [2]. The SGAN optimization comprises the generator and the discriminator with respective loss functions. The generator \(G\) accepts high-dimensional noise \(\mathbf{z} \sim p_z\) as input and generates fake samples \(\mathbf{x} = G(\mathbf{z}) \sim p_g\). Unlike in VAEs, the latent space in GANs is not learnt, but assumed to be a standard parametric distribution, typically Gaussian or uniformly distributed in \(\mathbb{R}^{100}\) or \(\mathbb{R}^{128}\). The discriminator \(D\) accepts an input \(\mathbf{x}\), which could come from either the data distribution \(p_d\) as a sample from the dataset \(\mathcal{D}\), or the generator distribution \(p_g\) as the output of the generator. The discriminator outputs a value \(D(\mathbf{x})\), which serves as a measure of <em>how realistic</em> the input sample \(\mathbf{x}\) is. Effectively, the generator must learn a mapping from the noise distribution to the data distribution, whereas the discriminator must learn the optimal two-class classifier.</p> <p>The SGAN discriminator \(D_{\phi}\) and generator \(G_{\theta}\) are implemented as neural networks and their parameters \((\phi,\theta)\) are optimized as follows:</p> \[\min_{\theta} \left\{ \max_{\phi} \Big\{ \mathbb{E}_{\mathbf{x}\sim p_d} \left[\ln\left(D_{\phi}(\mathbf{x})\right)\right] + \mathbb{E}_{\mathbf{z}\sim p_z} \left[ \ln\left(1 - D_{\phi}(G_{\theta}(\mathbf{z}))\right) \right] \Big\} \right\}.\] <p>Ideally, the min-max problem can be solved simultaneously to attain the <em>Nash equilibrium</em> of the two-player game. In practice, however, alternating gradient ascent-descent algorithms are used to train the networks until a suitable convergence criterion is met.</p> <p>Over the past decade, numerous variants of GANs have been proposed with several successful applications. From an analytical standpoint, the discriminator in these variants can be viewed as approximating a <em>measure of dissimilarity</em> between \(p_d\) and \(p_g\). On the other hand, the generator can be viewed as minimizing the measure that the discriminator approximates. Almost all known GAN flavors consider either a divergence or an integral probability metric. We review important GANs under each category.</p> <h3 id="divergence-minimizing-gans">Divergence-minimizing GANs</h3> <p>GANs were originally designed to minimize the divergence between the <em>target</em> data distribution \(p_d\) and the generator distribution \(p_g\). For instance, the SGAN formulation corresponds to optimizing the Jensen-Shannon divergence, whereas the least-squares GAN (LSGAN) [3] has been shown to optimize the Pearson-\(\chi^2\) divergence under certain assumptions. The \(f\)-GAN formulation [4] is a generalization to any choice of \(f\)-divergence, while other works [5] proposed extensions to various Bregman divergences. In the original SGAN, the discriminator output is constrained to the interval \([0,1]\), representing the probability of the input sample being real or fake, whereas LSGAN requires the discriminator output to match a set of chosen class-labels. In \(f\)-GANs, the discriminator output is real-valued, but an appropriately chosen activation function maps it to a desired interval.</p> <h3 id="integral-probability-metric-ipm-gans">Integral Probability Metric (IPM) GANs</h3> <p>The divergence metric approaches fail if \(p_d\) and \(p_g\) are of disjoint support [6], which shifted focus to <em>integral probability metrics</em> (IPMs). In these GAN flavors, the <em>discriminator</em> is replaced with a <em>real-valued critic</em> \(C\) that differentiates between the generator and data distributions in terms of an integral probability metric (IPM) defined over the class of critics to choose from. The choice of the class of critics gives rise to variants such as the Wasserstein GAN (WGAN) [7] with a Lipschitz-1 critic, the Fisher GAN [8] in which the second-order moments of the critic are bounded or Sobolev GANs [9], which favor critics with a finite energy in the gradient. The critic is a neural network similar to the discriminator, where the constraints are enforced appropriately, either by means of an adjustment of the network weights [7,10,11], or through a suitable penalty incorporated into the loss function [12,13,14]. Throughout this thesis, we use the term <em>discriminator</em> \(D(\mathbf{x})\) to refer to either a divergence-based discriminator or the IPM-based critic with the context and GAN loss function resolving any ambiguity.</p> <h2 id="wasserstein-autoencoders">Wasserstein Autoencoders</h2> <p>Wasserstein autoencoders (WAEs) [15] and adversarial autoencoders (AAE) [16] lie at the intersection of AEs and GANs. In these models, a vanilla autoencoder’s [17,18] latent space representation is required to conform to a given <em>prior</em> distribution, usually a Gaussian or a mixture of Gaussians, through an auxiliary network that minimizes the distance between the two distributions. While VAEs achieve this by means of a loss defined via an evidence lower bound, in AAEs and WAEs, the GAN formulation is leveraged.</p> <p>The <em>encoder-decoder</em> pair is trained by minimizing an appropriate distance measured between the input data distribution and that of the reconstructed samples. In the WAE-GAN setting, the encoder of the WAE also plays the role of the GAN generator, which is trained using a discriminator network to force the latent space distribution to match the prior using a suitable GAN loss. Considering the Euclidean distance metric between a data sample \(\mathbf{x}\) and the corresponding reconstruction \(\tilde{\mathbf{x}}\), and the SGAN loss, we obtain the AAE formulation. The vanilla WAE-GAN formulation employs the Euclidean loss for the reconstruction in combination with the KL divergence for the GAN loss. Sliced WAE [19] extended the framework to accommodate the sliced Wasserstein loss [20]. As an alternative to the adversarial formulation, maximum mean discrepancy (MMD) based variations of the WAE have also gained popularity, the most recent of which, the Cramér-Wold autoencoder (CWAE) [21] presents a characteristic kernel that allows for closed-form computation of the distance between the latent distribution and a standard Gaussian. Optimal transport based approaches either approximate the semi-discrete latent-space transportation map with the continuous Bernier potential by drawing links between the latent-space matching in WAEs and the Monge-Ampère partial differential equation [22,23] or determine the Kantorovich potential in a two-step manner to learn a mapping from the source to the target using a WGAN-GP discriminator [24]. Adding regularizers to the discriminator cost in AAEs has been shown to improve the interpolation capabilities of the AE [25].</p> <h2 id="a-deep-dive-into-gans">A Deep Dive into GANs</h2> <p>Divergence-minimizing GANs consider a discriminator loss that approximates a divergence between \(p_d\) and \(p_g\). Table 1 presents the discriminator and generator loss functions of various divergence-minimizing GANs. The standard GAN (SGAN) proposed by Goodfellow et al. (2014) [1] considers both saturating and non-saturating losses. The term saturation refers to the generator gradients vanishing during training. The vanilla SGAN (employing the saturating loss) results in a min-max zero-sum game where the optimal generator minimizes the Jensen-Shannon divergence between \(p_d\) and \(p_g\). On the contrary, the SGAN with a non-saturating loss (SGAN-NS) does not readily map to a divergence, but results in a superior performance when training the generator in practice. In the least-squares GAN (LSGAN) [3], one minimizes the squared distance between the discriminator prediction and the class labels \((a,b,c)\) for fake, real, and generated samples, respectively. The loss is not symmetric – the discriminator is trained to assign a class label \(a\) to the <em>fakes</em> and a class label \(b\) to the <em>reals</em>, while the generator loss is designed to promote <em>confusion</em> by assigning a label \(c\) to both the real and fake samples. The optimization objective can be seen as minimizing the Pearson-\(\chi^2\) divergence when \(b-c=1\) and \(b-a=2\).</p> <p>Nowozin et al. (2016) [4] considered \(f\)-divergences of the form:</p> \[\mathfrak{D}_f(p_d\|p_g) = \int_\mathcal{X} p_g(\mathbf{x}) f\left(\frac{p_d(\mathbf{x})}{p_g(\mathbf{x})} \right)\,d\mathbf{x},\] <p>where the divergence function \(f: \mathbb{R}_+ \rightarrow \mathbb{R}\) is convex, lower-semicontinuous and satisfies \(f(1) = 0\), and \(\mathcal{X}\) is a suitable domain of integration. They demonstrated that, in a GAN setting, the minimization of \(\mathfrak{D}_f(p_g\|p_d)\) is equivalent to the sequential minimization of the discriminator and generator losses:</p> \[\mathcal{L}_D^{f} = -\mathbb{E}_{\mathbf{x} \sim p_d}[T(\mathbf{x})] + \mathbb{E}_{\mathbf{x} \sim p_g}[f^{c}(T(\mathbf{x}))]\text{, and}\] \[\mathcal{L}_G^{f}= \mathbb{E}_{\mathbf{x} \sim p_{d}}[T^*(\mathbf{x})]-\mathbb{E}_{\mathbf{x} \sim p_g}[f^{c}(T^*(\mathbf{x}))] \text{,}\] <p>with respect to \(D\) and \(p_g\), respectively, where \(T(\mathbf{x})\) is the output of the discriminator subjected to activation \(g\), that is, \(T(\mathbf{x}) = g(D(\mathbf{x}))\) and \(T^*(\mathbf{x}) = g(D^*(\mathbf{x}))\), where \(D^*(\mathbf{x})\) is the optimal discriminator, and \(f^{c}\) is the Fenchel conjugate of \(f\). The choice of the divergence \(f\) and the activation \(g\) gives rise to GANs that minimize divergences such as Kullback-Leibler (KL), reverse KL, Pearson-\(\chi^2\), squared Hellinger, Jensen-Shannon, etc.</p> <h3 id="table-1-divergence-minimizing-gan-losses">Table 1: Divergence-Minimizing GAN Losses</h3> <style>.table-wrapper table td:first-child{width:20%}</style> <table> <thead> <tr> <th>GAN Variant</th> <th>Discriminator Loss: \(\mathcal{L}_D\)</th> <th>Generator Loss: \(\mathcal{L}_G\)</th> </tr> </thead> <tbody> <tr> <td><strong>SGAN</strong></td> <td>\(\mathcal{L}_D^{\mathrm{S}} = -\mathbb{E}_{\mathbf{x} \sim p_d}[\ln D(\mathbf{x})]-\mathbb{E}_{\mathbf{x} \sim p_g}[\ln (1-D(\mathbf{x}))]\)</td> <td>\(\mathcal{L}_G^{\mathrm{S}} = \mathbb{E}_{\mathbf{x} \sim p_d}[\ln D^*(\mathbf{x})]+\mathbb{E}_{\mathbf{x} \sim p_g}[\ln (1-D^*(\mathbf{x}))]\)</td> </tr> <tr> <td><strong>SGAN-NS</strong></td> <td>\(\mathcal{L}_D^{\mathrm{NS}} = -\mathbb{E}_{\mathbf{x} \sim p_d}[\ln D(\mathbf{x})]-\mathbb{E}_{\mathbf{x} \sim p_g}[\ln (1-D(\mathbf{x}))]\)</td> <td>\(\mathcal{L}_G^{\mathrm{NS}} = -\mathbb{E}_{\mathbf{x} \sim p_d}[\ln (1-D^*(\mathbf{x}))]-\mathbb{E}_{\mathbf{x} \sim p_g}[\ln D^*(\mathbf{x})]\)</td> </tr> <tr> <td><strong>LSGAN</strong></td> <td>\(\mathcal{L}_D^{\mathrm{LS}} =\mathbb{E}_{\mathbf{x} \sim p_d}[(D(\mathbf{x})-b)^2]+\mathbb{E}_{\mathbf{x} \sim p_g}[(D(\mathbf{x})-a)^2]\)</td> <td>\(\mathcal{L}_G^{\mathrm{LS}} =\mathbb{E}_{\mathbf{x} \sim p_d}[(D^*(\mathbf{x})-c)^2]+\mathbb{E}_{\mathbf{x} \sim p_g}[(D^*(\mathbf{x})-c)^2]\)</td> </tr> <tr> <td><strong>\(f\)-GAN</strong></td> <td>\(\mathcal{L}_D^{f} =-\mathbb{E}_{\mathbf{x} \sim p_d}[g(D(\mathbf{x}))] + \mathbb{E}_{\mathbf{x} \sim p_g}[f^{c}(g(D(\mathbf{x})))]\)</td> <td>\(\mathcal{L}_G^{f} =\mathbb{E}_{\mathbf{x} \sim p_d}[g(D^*(\mathbf{x}))]-\mathbb{E}_{\mathbf{x} \sim p_g}[f^{c}(g(D^*(\mathbf{x})))]\)</td> </tr> </tbody> </table> <p><strong>Table 1:</strong> A summary of various divergence minimizing GAN losses. The SGAN and \(f\)-GAN losses are symmetric and lead to a min-max optimization problem, whereas the LSGAN and SGAN-NS are not symmetric. The LSGAN loss is parametrized by class labels \((a,b,c)\), wherein the discriminator learns a classifier with the class label \(b\) for the <em>reals</em> and a class label \(a\) for the <em>fakes</em>. The LSGAN generator is trained to <em>confuse</em> the discriminator by enforcing the same class label \(c\) to both the reals and the fakes. Various choices of the Csiszár divergence function \(f\) (and consequently, its conjugate \(f^c\)), and <em>activation</em> function \(g\) give rise to the family of \(f\)-GANs.</p> <h3 id="ipm-based-gans-and-regularization">IPM-Based GANs and Regularization</h3> <p>The divergence metric approaches fail if \(p_d\) and \(p_g\) are of disjoint support [6], which shifted focus to <em>integral probability metrics</em> (IPMs), where a <em>critic</em> function is chosen to approximate a chosen IPM between the distributions [7,8]. Choosing the IPM is equivalent to constraining the class of functions from which the critic/discriminator is drawn. The most popular variant, inspired by optimal transport, is the Wasserstein GAN (WGAN) [7], in which the objective is to minimize the Wasserstein-1 or <em>earth mover’s</em> distance between \(p_d\) and \(p_g\), and the critic is constrained to be Lipschitz-1. Gulrajani et al. (2017) [12] enforced a first-order gradient penalty on the discriminator network to approximate the Lipschitz constraint. Here, the loss is defined via the Kantorovich–Rubinstein duality, subjected to a regularizer \(\Omega_D\):</p> \[\mathcal{L}_D^W = \mathbb{E}_{\mathbf{x} \sim p_d}[D(\mathbf{x})]- \mathbb{E}_{\mathbf{x} \sim p_g}[D(\mathbf{x})] + \lambda_d\Omega_D,\] <p>where \(\lambda_d\) is the Lagrange multiplier associated with the regularizer. Table 2 summarizes the IPM-GAN losses, the corresponding constraint space of the discriminator, and the ideal form of the regularizer \(\Omega_D\). However, these regularizers are usually replaced with an implementable counterpart, typically involving first-order gradient-based terms [12,14,31,32]. On the other hand, several works [13,27,28,33] showed the empirical success of the first-order gradient penalty in divergence-minimizing GANs. Cramér GANs [34] and Sobolev GANs [9,29] bound the energy in the gradients of \(D\), enforcing Sobolev-space constraints.</p> <h3 id="table-2-ipm-gan-losses">Table 2: IPM-GAN Losses</h3> <style>.table-wrapper table td:first-child{width:20%}</style> <table> <thead> <tr> <th>GAN Variant</th> <th>Unregularized Discriminator Loss</th> <th>Closed-form Metric: \(\mathfrak{D}\)</th> <th>Discriminator Constraint Space</th> </tr> </thead> <tbody> <tr> <td><strong>\(f\)-GANs</strong></td> <td>\(-\mathbb{E}_{\mathbf{x} \sim p_d}[T(\mathbf{x})] +\mathbb{E}_{\mathbf{x} \sim p_g}[f^{c}(T(\mathbf{x}))]\)</td> <td>\(\mathbb{E}_{\mathbf{x}\sim p_g} \left[ f\left( \frac{p_d(\mathbf{x})}{p_g(\mathbf{x})}\right) \right]\)</td> <td>\(\{T = g(D(\cdot)):\,\mathcal{X} \rightarrow \mathbb{R}\,;\,T\in\mathrm{dom}(f)\}\)</td> </tr> <tr> <td><strong>WGAN</strong></td> <td>\(\mathbb{E}_{\mathbf{x} \sim p_d}[D(\mathbf{x})]- \mathbb{E}_{\mathbf{x} \sim p_g}[D(\mathbf{x})]\)</td> <td>—-</td> <td>\(\{D:\,\mathcal{X} \rightarrow \mathbb{R}\,;\,|D|_{\mathrm{Lip}}\leq 1 \}\)</td> </tr> <tr> <td><strong>\(\mu\)-Fisher-GAN</strong></td> <td>\(\mathbb{E}_{\mathbf{x} \sim p_d}[D(\mathbf{x})]- \mathbb{E}_{\mathbf{x} \sim p_g}[D(\mathbf{x})]\)</td> <td>\(\sqrt{ \mathbb{E}_{\mathbf{x}\sim\mu(\mathbf{x})} \left[\left(\frac{p_d(\mathbf{x}) - p_g(\mathbf{x})}{\mu(\mathbf{x})}\right)^2 \right]}\)</td> <td>\(\left\{D:\,\mathcal{X}\rightarrow \mathbb{R}\,;\,\mathbb{E}_{\mathbf{x}\sim\mu} \left[D^2(\mathbf{x})\right]\leq 1\right\}\)</td> </tr> <tr> <td><strong>Cramér-GAN</strong></td> <td>\(\mathbb{E}_{\mathbf{x} \sim p_d}[D(\mathbf{x})]- \mathbb{E}_{\mathbf{x} \sim p_g}[D(\mathbf{x})]\)</td> <td>\(\sqrt{ \mathbb{E}_{x\sim p_g} \left[\left(\frac{\mathrm{CDF}_{p_d}(x) - \mathrm{CDF}_{p_g}(x)}{p_d(x)}\right)^2 \right]}\)</td> <td>\(\left\{D:\,\mathcal{X}\rightarrow \mathbb{R}\,;\,\mathbb{E}_{x\sim p_d} \left[ \left(\frac{\partial D(x)}{\partial x}\right)^2\right]\leq 1\right\}\)</td> </tr> <tr> <td><strong>\(\mu\)-Sobolev-GAN</strong></td> <td>\(\mathbb{E}_{\mathbf{x} \sim p_d}[D(\mathbf{x})]- \mathbb{E}_{\mathbf{x} \sim p_g}[D(\mathbf{x})]\)</td> <td>\(\sqrt{ \mathbb{E}_{\mathbf{x}\sim\mu(\mathbf{x})} \left[ \sum_{i=1}^n\left(\frac{\mathrm{CDF}_{i,p_d}(\mathbf{x}) - \mathrm{CDF}_{i,p_g}(\mathbf{x})}{n\,\mu(\mathbf{x})}\right)^2 \right]}\)</td> <td>\(\left\{D:\,\mathcal{X}\rightarrow \mathbb{R}\,;\,\mathbb{E}_{\mathbf{x}\sim\mu} \left[ |\nabla_{\mathbf{x}} D(\mathbf{x})|_2^2\right]\leq 1\right\}\)</td> </tr> <tr> <td><strong>MMD-GANs</strong></td> <td>\(\mathbb{E}_{\mathbf{x} \sim p_d}[D(\mathbf{x})]- \mathbb{E}_{\mathbf{x} \sim p_g}[D(\mathbf{x})]\)</td> <td>\(\left| \mathbb{E}_{\mathbf{x}\sim p_g} \left[\phi_{k}(\mathbf{x})\right] - \mathbb{E}_{\mathbf{x}\sim p_d} \left[\phi_{k}(\mathbf{x})\right] \right|_{\mathscr{H}_{\phi_{k}}}\)</td> <td>\(\{D:\,\mathcal{X} \rightarrow \mathbb{R}\,;\,|D|_{\mathscr{H}_{\phi_{k}}} \leq 1\}\)</td> </tr> </tbody> </table> <p><strong>Table 2:</strong> A summary of various IPM-GAN losses considered in the literature, compared against an \(f\)-GAN loss. Various choices of the \(f\)-divergence function give rise to the family of \(f\)-GANs. Other GAN variants consider the standard Wasserstein-1 IPM, with varying choices for the discriminator regularizer (the <em>closed-form metric</em>), which in turn induces a constraint space for the discriminator. WGAN enforces a Lipschitz-1 discriminator by means of weight-clipping on the network, while Fisher GANs draw discriminators with finite \(L_2\) norms. While Cramér GANs enforce a gradient-norm penalty in 1-D with a metric defined over the cumulative density functions (CDF), Sobolev GANs consider a generalization involving the sum of all marginal CDFs (indexed by \(i\)). Maximum mean discrepancy GANs (MMD-GANs) derived a kernel-mean statistic associated with the RKHS \(\mathscr{H}_{\phi_{k}}\).</p> <h3 id="navigation">Navigation</h3> <ul> <li><strong>Previous:</strong> <a href="/thesis-chapters/2023/05/10/thesis-chapter-1p1-IntroGenMod.html">Chapter 1.1: Introduction to Generative Modeling</a></li> <li><strong>Next:</strong> <a href="/thesis-chapters/2023/05/10/thesis-chapter-1p3-FlowsDiffusion.html">Chapter 1.3: Flow-based and Diffusion Models</a></li> <li><strong>Back to:</strong> <a href="/projects/1_thesis/">Thesis Project</a></li> </ul> <hr/> <h2 id="references">References</h2> <ol> <li> <p><strong>Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., &amp; Bengio, Y. (2014).</strong> Generative adversarial nets. In <em>Advances in Neural Information Processing Systems 27 (NIPS 2014)</em> (pp. 2672-2680).</p> </li> <li> <p><strong>Liu, M.-Y., Huang, X., Yu, J., Wang, T.-C., &amp; Mallya, A. (2021).</strong> Generative adversarial networks for image and video synthesis: Algorithms and applications. <em>Proceedings of the IEEE</em>, 109(5), 839-862.</p> </li> <li> <p><strong>Mao, X., Li, Q., Xie, H., Lau, R. Y., Wang, Z., &amp; Smolley, S. P. (2017).</strong> Least squares generative adversarial networks. In <em>International Conference on Computer Vision (ICCV)</em> (pp. 2813-2821).</p> </li> <li> <p><strong>Nowozin, S., Cseke, B., &amp; Tomioka, R. (2016).</strong> f-GAN: Training generative neural samplers using variational divergence minimization. In <em>Advances in Neural Information Processing Systems 29 (NIPS 2016)</em> (pp. 271-279).</p> </li> <li> <p><strong>Tao, C., Chen, L., Henao, R., Feng, J., &amp; Carin, L. (2018).</strong> Chi-square generative adversarial network. In <em>International Conference on Machine Learning (ICML)</em> (pp. 4843-4852).</p> </li> <li> <p><strong>Arjovsky, M., &amp; Bottou, L. (2017).</strong> Towards principled methods for training generative adversarial networks. In <em>International Conference on Learning Representations (ICLR)</em>.</p> </li> <li> <p><strong>Arjovsky, M., Chintala, S., &amp; Bottou, L. (2017).</strong> Wasserstein generative adversarial networks. In <em>International Conference on Machine Learning (ICML)</em> (pp. 214-223).</p> </li> <li> <p><strong>Mroueh, Y., &amp; Sercu, T. (2017).</strong> Fisher GAN. In <em>Advances in Neural Information Processing Systems 30 (NIPS 2017)</em> (pp. 2513-2523).</p> </li> <li> <p><strong>Mroueh, Y., Sercu, T., &amp; Goel, V. (2018).</strong> Sobolev GAN. In <em>International Conference on Learning Representations (ICLR)</em>.</p> </li> <li> <p><strong>Li, Z., Hu, Z., Luo, S., Chen, S., Gao, K., Chen, C., Yang, J., &amp; Xu, B. (2019).</strong> Spectral normalization for generative adversarial networks. In <em>International Conference on Learning Representations (ICLR)</em>.</p> </li> <li> <p><strong>Wang, D., Gong, Z., &amp; Liu, Q. (2016).</strong> Stein variational gradient descent as gradient flow. In <em>Advances in Neural Information Processing Systems 29 (NIPS 2016)</em>.</p> </li> <li> <p><strong>Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., &amp; Courville, A. (2017).</strong> Improved training of Wasserstein GANs. In <em>Advances in Neural Information Processing Systems 30 (NIPS 2017)</em> (pp. 5767-5777).</p> </li> <li> <p><strong>Kodali, N., Abernethy, J., Hays, J., &amp; Kira, Z. (2017).</strong> On convergence and stability of GANs. <em>arXiv preprint arXiv:1705.07215</em>.</p> </li> <li> <p><strong>Mescheder, L., Geiger, A., &amp; Nowozin, S. (2018).</strong> Which training methods for GANs do actually converge? In <em>International Conference on Machine Learning (ICML)</em> (pp. 3481-3490).</p> </li> <li> <p><strong>Tolstikhin, I., Bousquet, O., Gelly, S., &amp; Schoelkopf, B. (2018).</strong> Wasserstein auto-encoders. In <em>International Conference on Learning Representations (ICLR)</em>.</p> </li> <li> <p><strong>Makhzani, A., Shlens, J., Jaitly, N., Goodfellow, I., &amp; Frey, B. (2015).</strong> Adversarial autoencoders. <em>arXiv preprint arXiv:1511.05644</em>.</p> </li> <li> <p><strong>Hinton, G. E., &amp; Zemel, R. S. (1994).</strong> Autoencoders, minimum description length and Helmholtz free energy. In <em>Advances in Neural Information Processing Systems 6 (NIPS 1993)</em> (pp. 3-10).</p> </li> <li> <p><strong>Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016).</strong> <em>Deep Learning.</em> MIT Press.</p> </li> <li> <p><strong>Kolouri, S., Pope, P. E., Martin, C. E., &amp; Rohde, G. K. (2019).</strong> Sliced Wasserstein auto-encoders. In <em>International Conference on Learning Representations (ICLR)</em>.</p> </li> <li> <p><strong>Kolouri, S., Zou, Y., &amp; Rohde, G. K. (2018).</strong> Sliced Wasserstein kernels for probability distributions. In <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 5258-5267).</p> </li> <li> <p><strong>Patrini, G., van den Berg, R., Forré, P., Carioni, M., Bhargav, S., Welling, M., Genewein, T., &amp; Roth, F. (2020).</strong> Sinkhorn autoencoders. In <em>Conference on Uncertainty in Artificial Intelligence (UAI)</em> (pp. 733-743).</p> </li> <li> <p><strong>Fan, L., Dai, W., Lee, D.-J., &amp; Liu, W. (2019).</strong> On the effectiveness of optimal transport for generative modeling. <em>arXiv preprint arXiv:1901.11373</em>.</p> </li> <li> <p><strong>Patrini, G., van den Berg, R., Forré, P., Carioni, M., Bhargav, S., Welling, M., Genewein, T., &amp; Roth, F. (2020).</strong> Optimal transport autoencoders. In <em>International Conference on Learning Representations (ICLR)</em>.</p> </li> <li> <p><strong>Rubenstein, P., Schoelkopf, B., &amp; Tolstikhin, I. (2018).</strong> On the latent space of Wasserstein auto-encoders. <em>arXiv preprint arXiv:1802.03761</em>.</p> </li> <li> <p><strong>Ghosh, P., Sajjadi, M. S. M., Vergari, A., Black, M., &amp; Schölkopf, B. (2019).</strong> From variational to deterministic autoencoders. <em>arXiv preprint arXiv:1903.12436</em>.</p> </li> <li> <p><strong>Peyré, G., &amp; Cuturi, M. (2019).</strong> Computational optimal transport. <em>Foundations and Trends in Machine Learning</em>, 11(5-6), 355-607.</p> </li> <li> <p><strong>Kodali, N., Abernethy, J., Hays, J., &amp; Kira, Z. (2017).</strong> On convergence and stability of GANs. <em>arXiv preprint arXiv:1705.07215</em>.</p> </li> <li> <p><strong>Kodali, N., Hays, J., Abernethy, J., &amp; Kira, Z. (2017).</strong> How to train your DRAGAN. <em>arXiv preprint arXiv:1705.07215</em>.</p> </li> <li> <p><strong>Adler, J., &amp; Lunz, S. (2018).</strong> Banach Wasserstein GAN. In <em>Advances in Neural Information Processing Systems 31 (NeurIPS 2018)</em> (pp. 6754-6763).</p> </li> <li> <p><strong>Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., &amp; Courville, A. (2017).</strong> Improved training of Wasserstein GANs. In <em>Advances in Neural Information Processing Systems 30 (NIPS 2017)</em> (pp. 5767-5777).</p> </li> <li> <p><strong>Petzka, H., Fischer, A., &amp; Lukovnikov, D. (2018).</strong> On the regularization of Wasserstein GANs. In <em>International Conference on Learning Representations (ICLR)</em>.</p> </li> <li> <p><strong>Patel, A. B., Nguyen, T., &amp; Baraniuk, R. G. (2020).</strong> A probabilistic framework for deep learning. In <em>Advances in Neural Information Processing Systems 29 (NIPS 2016)</em>.</p> </li> <li> <p><strong>Mescheder, L., Nowozin, S., &amp; Geiger, A. (2018).</strong> The numerics of GANs. In <em>Advances in Neural Information Processing Systems 31 (NeurIPS 2018)</em> (pp. 1825-1835).</p> </li> <li> <p><strong>Bellemare, M. G., Danihelka, I., Dabney, W., Mohamed, S., Lakshminarayanan, B., Hoyer, S., &amp; Munos, R. (2017).</strong> The Cramer distance as a solution to biased Wasserstein gradients. <em>arXiv preprint arXiv:1705.10743</em>.</p> </li> </ol>]]></content><author><name></name></author><category term="thesis-chapters"/><category term="thesis"/><category term="GANs"/><category term="machine-learning"/><category term="generative-models"/><summary type="html"><![CDATA[An introduction to GANs and Wasserstein Autoencoders, covering the foundational concepts and variants]]></summary></entry><entry><title type="html">Thesis Chapter 1.1: Introduction to Generative Modeling</title><link href="https://www.siddarthasokan.com/thesis-chapters/2023/05/10/thesis-chapter-1p1-introGenMod.html" rel="alternate" type="text/html" title="Thesis Chapter 1.1: Introduction to Generative Modeling"/><published>2023-05-10T00:00:00+00:00</published><updated>2023-05-10T00:00:00+00:00</updated><id>https://www.siddarthasokan.com/thesis-chapters/2023/05/10/thesis-chapter-1p1-introGenMod</id><content type="html" xml:base="https://www.siddarthasokan.com/thesis-chapters/2023/05/10/thesis-chapter-1p1-introGenMod.html"><![CDATA[<blockquote> <p><em>What I cannot create, I do not understand.</em><br/> — Richard P. Feynman</p> </blockquote> <h2 id="generative-models">Generative Models</h2> <p>Generative modeling is a branch of machine learning that addresses the problem of learning the distribution of given data samples, which can subsequently be used to create more such samples. Generative models have gained immense popularity in the generation of images [1], text [2], and audio [3] and learning multi-modal transformations [4]. In this thesis, we focus on image datasets, and consider generative modelling for images. Consider the dataset \(\mathcal{D} = \big\{\mathbf{x}_i~\|~\mathbf{x}_i\in\mathbb{R}^n,\,i=1,2,\ldots,N\big\}\), with samples drawn from an unknown distribution \(p_d\) (the <em>data distribution</em>). We are considering \(\mathbf{x}_i\) to be the vectorized representation of an image, with \(n = h\times w\times c\), where \((h,w,c)\) denote the height, width, and the number of channels in the image. This representation also allows one to consider multichannel images, such as color images, multispectral images, and hyperspectral images.</p> <p>Neural-network-based generative models output samples \(\tilde{\mathbf{x}}\) that are governed by a parametric distribution \(p_{\text{model}}(\tilde{\mathbf{x}};\theta)\). The goal in optimizing/training the neural network model is to learn the optimal parameters \(\theta^*\) such that the generated samples appear realistic, and \(p_{\text{model}}(\tilde{\mathbf{x}};\theta^*) = p_d\). Early neural-network-based generative models, going back to the 1980s, are energy-based models (EBMs) that define an energy functional on data samples. However, these models relied on Markov chain Monte Carlo (MCMC) for sampling [5], and could not be scaled to high-dimensional objects such as images.</p> <p>Modern approaches to generative modelling rely on the <em>Manifold Hypothesis</em> [6], which posits that although the ambient dimension of images is large \((\mathbf{x}\in\mathbb{R}^n)\), they actually reside in a much lower-dimensional manifold \((\mathbf{z}\in\mathbb{R}^d)\) with \(d\ll n\), often several orders lower. Leveraging this property, generative models have targeted nearly invertible low-dimensional <em>latent</em> representations \(\{\mathbf{z}_i\}\) of images \(\{\mathbf{x}_i\}\). Autoencoders (AEs) [7], variational autoencoders (VAEs) [8], Wasserstein autoencoders (WAEs) [9], generative adversarial networks (GANs) [10], and variants thereof, are all examples of such generative models. By contrast, advanced frameworks such as normalizing flows (NFs) [11] and diffusion models (DMs) [13] operate on the ambient dimensionality of the data and are far more challenging to train and intensive in terms of memory and compute. Our focus in this thesis is predominantly on GANs, with occasional connections to WAEs, NFs, and DMs.</p> <p>We briefly introduce the various generative models considered in this thesis.</p> <h3 id="autoencoders">Autoencoders</h3> <p>Autoencoders comprise a pair of neural networks, namely the encoder \((\text{Enc}_{\phi})\) and decoder \((\text{Dec}_{\theta})\), trained to learn a compressed (low-dimensional) representation of the data \(\mathcal{D}\). Given a sample \(\mathbf{x}_i\), the encoder learns a latent representation of the data \(\mathbf{z}_i = \text{Enc}_{\phi}(\mathbf{x}_i)\), while the decoder learns to invert the encoder transformation, \(\tilde{\mathbf{x}}_i = \text{Dec}_{\theta}(\mathbf{z}_i)\) such that \(\tilde{\mathbf{x}}_i \approx \mathbf{x}_i\). While the encoder and decoder networks are typically fully-connected multi-layer perceptron (MLP) networks, convolutional AEs (CAEs) consider encoders with convolutional layers, while the decoders consist of transposed convolution layers. While autoencoders were originally proposed as the nonlinear counterpart of principal component analysis (PCA), in the recent literature, AEs have been leveraged to implement flow-based and generative pre-training architectures.</p> <p>The standard AE optimization involves training the encoder-decoder pair to generate accurate reconstructions. Given the dataset \(\mathcal{D}\), the \(N\)-sample estimate of the AE loss is given by</p> \[\mathcal{L}^{\text{AE}}(\phi,\theta) = \frac{1}{N}\sum_{i=1,\,\mathbf{x}_i\sim\mathcal{D}}^{N}\text{dist}\left(\mathbf{x}_i,\tilde{\mathbf{x}}_i \right) = \frac{1}{N}\sum_{i=1,\,\mathbf{x}_i\sim\mathcal{D}}^{N}\text{dist}\left(\mathbf{x}_i,\text{Dec}_{\theta}\left( \text{Enc}_{\phi}\left(\mathbf{x}_i \right) \right) \right),\] <p>where in turn, \(\phi^*\) and \(\theta^*\) denote the optimal encoder and decoder parameters:</p> \[(\phi^*,\theta^*) = \arg\min_{\theta,\phi} \left\{ \mathcal{L}^{\text{AE}} \right\}.\] <p>Typical choices for the distance measure include the \(\ell_1\) or \(\ell_2\) loss:</p> \[\mathcal{L}_{\text{L}_p}^{\text{AE}}(\phi,\theta) = \frac{1}{N}\sum_{i=1,\,\mathbf{x}_i\sim\mathcal{D}}^{N}\\|\mathbf{x}_i - \text{Dec}_{\theta}\left( \text{Enc}_{\phi}\left(\mathbf{x}_i \right) \right) \\|_{\ell_p},\] <p>where \(p = 1\) or \(2\), respectively, or a weighted combination of these. Regularized autoencoders have also been considered, wherein the encoder and decoder networks are regularized to either be smooth transformations [12] or learn sparse representations [15]. Although autoencoders are generative models in the sense that the decoders learn to output realistic images given a latent representation, a major limitation is their inability to generate new/unseen images. This can be attributed to the lack of a closed-form or parametric structure to the latent-space distribution, which results in inferior reconstructions of latent-space interpolations. The variational and Wasserstein autoencoder schemes alleviate this issue, by enforcing constraints on the distribution of the latent representations.</p> <h3 id="variational-autoencoders">Variational Autoencoders</h3> <p>While AEs consider the encoder output as a <em>deterministic</em> representation of the input image, in variational autoencoders (VAEs) [8], the latent representations are assumed to be random, with the objective of modelling the joint distribution \(p_{\text{model}}(\mathbf{x},\mathbf{z})\), from which the data distribution can be computed as \(p_d(\mathbf{x}) = \int_{\mathcal{Z}} p_{\text{model}}(\mathbf{x},\mathbf{z})\,d\mathbf{z}\), where \(\mathcal{Z}\) denotes the support of the latent-space distribution. Invoking Bayes’ rule, we have \(p_{\text{model}}(\mathbf{x},\mathbf{z}) = \frac{p_{\text{model}}(\mathbf{x}\|\mathbf{z}) p_z(\mathbf{z})}{p_{\text{model}}(\mathbf{z}\|\mathbf{x})}\). The decoder can be interpreted as approximating the posterior, <em>i.e.</em>, \(\text{Dec}_{\theta}(\mathbf{z}) \sim p_{\text{model}}(\mathbf{x}\|\mathbf{z})\). As the underlying data distribution is inaccessible, the encoder models an approximation of the likelihood, \(\text{Enc}_{\phi}(\mathbf{x}) \sim q(\mathbf{z}\|\mathbf{x}) \approx p_{\text{model}}(\mathbf{z}\|\mathbf{x})\). In VAEs, one optimizes the so called evidence lower bound (ELBO), given by</p> \[\mathcal{L}_{\text{ELBO}}^{\text{VAE}} = \underbrace{\mathbb{E}_{\mathbf{z}\sim \text{Enc}_{\phi}(\mathbf{x})} \left[ \ln\left(p_{\text{model}}(\mathbf{x}\|\mathbf{z})\right)\right]}_{\text{reconstruction error}} - \underbrace{D_{\text{KL}}\left( q(\mathbf{z}\|\mathbf{x}) \,\\|\, p_z(\mathbf{z})\right)}_{\text{prior matching error}},\] <p>where \(D_{\text{KL}}\) denotes the Kullback-Leibler (KL) divergence between two distributions. The reconstruction error, as in the case of AEs, can be approximated in terms of the \(\ell_2\) or \(\ell_1\) loss. The KL divergence can be further simplified by assuming a Gaussian model \(p_z(\mathbf{z})\), with the latent sample defined by means of a <em>change-of-variable</em> formula as \(\mathbf{z}_i = \mu_{\phi}(\mathbf{x}_i)+ \sigma_{\phi}(\mathbf{x}_i)\odot\boldsymbol{\epsilon}\), where \((\mu_{\phi},\sigma_{\phi})= \text{Enc}_{\phi}(\mathbf{x}_i)\) and \(\boldsymbol{\epsilon}\) is drawn from the standard normal distribution, <em>i.e.</em>, \(\boldsymbol{\epsilon}\sim\mathcal{N}(\mathbf{0}_d,\mathbb{I}_d)\), where in turn, \(\mathbf{0}_d\) denotes the \(d\)-dimensional null vector, \(\mathbb{I}_d\) is the \(d\)-dimensional identity matrix, and \(\odot\) stands for pointwise multiplication. Although VAEs are superior to AEs in terms of enforcing structure on the latent space, generating novel unseen samples remains challenging. Further, the use of the \(\ell_2\) cost results in over-smoothing of the reconstructed images, owing to the implicit Gaussian prior that this choice of loss introduces [16]. Generative adversarial networks (GANs) [10] were proposed as an alternative to VAEs, to alleviate its limitations.</p> <h3 id="navigation">Navigation</h3> <ul> <li><strong>Next:</strong> <a href="/thesis-chapters/2023/05/10/thesis-chapter-1p2-IntroGANs.html">Chapter 1.2: Generative Adversarial Networks</a></li> <li><strong>Back to:</strong> <a href="/projects/1_thesis">Thesis Project</a></li> </ul> <hr/> <h2 id="references">References</h2> <ol> <li> <p><strong>Kang, M., Zhu, J.-Y., Zhang, R., Park, J., Shechtman, E., Paris, S., &amp; Park, T. (2023).</strong> Scaling up GANs for text-to-image synthesis. In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>.</p> </li> <li> <p><strong>Guo, J., Lu, S., Cai, H., Zhang, W., Yu, Y., &amp; Wang, J. (2018).</strong> Long text generation via adversarial training with leaked information. <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>, 32.</p> </li> <li> <p><strong>Engel, J., Agrawal, K. K., Chen, S., Gulrajani, I., Donahue, C., &amp; Roberts, A. (2019).</strong> GANSynth: Adversarial neural audio synthesis. In <em>International Conference on Learning Representations (ICLR)</em>.</p> </li> <li> <p><strong>Yu, J., Xu, Y., Koh, J. Y., Luong, T., Baid, G., Wang, Z., Vasudevan, V., Ku, A., Yang, Y., Ayan, B. K., Hutchinson, B., Han, W., Parekh, A., Li, X., Zhang, H., Baldridge, J., &amp; Wu, Y. (2022).</strong> Scaling autoregressive models for content-rich text-to-image generation. <em>arXiv preprint arXiv:2206.10789</em>.</p> </li> <li> <p><strong>Kalos, M. H., &amp; Whitlock, P. A. (1986).</strong> <em>Monte Carlo Methods.</em> Wiley Publications.</p> </li> <li> <p><strong>Carlsson, G., Ishkhanov, T., de Silva, V., &amp; Zomorodian, A. (2008).</strong> On the local behavior of spaces of natural images. <em>International Journal of Computer Vision</em>, 76(1), 1-12.</p> </li> <li> <p><strong>Hinton, G. E., &amp; Zemel, R. S. (1994).</strong> Autoencoders, minimum description length and Helmholtz free energy. In <em>Advances in Neural Information Processing Systems 6 (NIPS 1993)</em> (pp. 3-10).</p> </li> <li> <p><strong>Kingma, D. P., &amp; Welling, M. (2013).</strong> Auto-encoding variational Bayes. <em>arXiv preprint arXiv:1312.6114</em>.</p> </li> <li> <p><strong>Tolstikhin, I., Bousquet, O., Gelly, S., &amp; Schoelkopf, B. (2018).</strong> Wasserstein auto-encoders. In <em>International Conference on Learning Representations (ICLR)</em>.</p> </li> <li> <p><strong>Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., &amp; Bengio, Y. (2014).</strong> Generative adversarial nets. In <em>Advances in Neural Information Processing Systems 27 (NIPS 2014)</em> (pp. 2672-2680).</p> </li> <li> <p><strong>Rezende, D., &amp; Mohamed, S. (2015).</strong> Variational inference with normalizing flows. In <em>International Conference on Machine Learning (ICML)</em> (pp. 1530-1538).</p> </li> <li> <p><strong>Liang, K., Chang, H., Cui, Z., Shan, S., &amp; Chen, X. (2015).</strong> Representation learning with smooth autoencoder. In <em>12th Asian Conference on Computer Vision</em> (pp. 72-86).</p> </li> <li> <p><strong>Ho, J., Jain, A., &amp; Abbeel, P. (2020).</strong> Denoising diffusion probabilistic models. In <em>Advances in Neural Information Processing Systems 33 (NeurIPS 2020)</em> (pp. 6840-6851).</p> </li> <li> <p><strong>Rifai, S., Mesnil, G., Vincent, P., Muller, X., Bengio, Y., Dauphin, Y., &amp; Glorot, X. (2011).</strong> Higher order contractive auto-encoder. In <em>Joint European Conference on Machine Learning and Knowledge Discovery in Databases</em> (pp. 645-660). Springer.</p> </li> <li> <p><strong>Ng, A. (2011).</strong> Sparse autoencoder. <em>CS294A Lecture Notes</em>, 72, 1-19.</p> </li> <li> <p><strong>Figueiredo, M. (2001).</strong> Adaptive sparseness using Jeffreys prior. In <em>Advances in Neural Information Processing Systems</em> 2001, 14.</p> </li> </ol>]]></content><author><name></name></author><category term="thesis-chapters"/><category term="thesis"/><category term="GANs"/><category term="machine-learning"/><category term="generative-models"/><summary type="html"><![CDATA[An introduction to generative modeling, covering autoencoders, VAEs, GANs, flows, and diffusion models]]></summary></entry></feed>