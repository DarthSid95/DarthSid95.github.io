<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://www.siddarthasokan.com/feed.xml" rel="self" type="application/atom+xml"/><link href="https://www.siddarthasokan.com/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-22T16:06:25+00:00</updated><id>https://www.siddarthasokan.com/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Thesis Chapter 1.4: An Introduction to Variational Calculus</title><link href="https://www.siddarthasokan.com/thesis-chapters-p0/2023/05/10/thesis-chapter-1p4-VariationalCalculus.html" rel="alternate" type="text/html" title="Thesis Chapter 1.4: An Introduction to Variational Calculus"/><published>2023-05-10T03:00:00+00:00</published><updated>2023-05-10T03:00:00+00:00</updated><id>https://www.siddarthasokan.com/thesis-chapters-p0/2023/05/10/thesis-chapter-1p4-VariationalCalculus</id><content type="html" xml:base="https://www.siddarthasokan.com/thesis-chapters-p0/2023/05/10/thesis-chapter-1p4-VariationalCalculus.html"><![CDATA[<blockquote> <p><em>What I cannot create, I do not understand.</em><br/> — Richard P. Feynman</p> </blockquote> <h2 id="calculus-of-variations">Calculus of Variations</h2> <p>The cornerstone of this thesis is the <strong>Euler-Lagrange (EL) framework</strong>, which is at the heart of <strong>Calculus of Variations</strong>. The EL conditions are of fundamental importance in solving several problems in physics and serve as a functional equivalent to the variable optimization performed in univariate or multivariate Calculus.</p> <p>Consider the 1-D integral cost</p> \[\mathcal{L}\left( y(x), y^{\prime}(x)\right)=\int \limits_{a}^b \mathcal{F}\left(x, y(x), y^{\prime}(x)\right)\,dx,\] <p>to be optimized with respect to \(y(x)\), \(x \in [a,b] \subset \mathbb{R}\), which is assumed to be continuously differentiable or at least continuous with a piecewise-smooth derivative \(y^{\prime} = \frac{dy}{dx}\), with finite Dirichlet boundary conditions (<em>i.e.</em>, the function vanishes on the boundary at \(x=a\) and \(x=b\)). Let \(y^*(x)\) denote the optimizer of \(\mathcal{L}\). The <em>first variation</em> of \(\mathcal{L}\), evaluated at \(y^*\), is defined as the derivative \(\delta\mathcal{L}(y^*;\eta) = \frac{\partial\mathcal{L}_{y,\epsilon}(\epsilon)}{\partial\epsilon}\) evaluated at \(\epsilon=0\), where \(\mathcal{L}_{y,\epsilon}(\epsilon)\) denotes an \(\epsilon\)-perturbation of the argument \(y\) about the optimum \(y^*\), given by</p> \[\begin{align*} \mathcal{L}_{y,\epsilon}(\epsilon) &amp;= \mathcal{L}\left( y^*(x) + \epsilon\,\eta(x), y^{*\prime}(x) + \epsilon \,\eta^{\prime}(x)\right) \\ &amp;= \int \limits_a^b \mathcal{F}\left(x, y^*(x) + \epsilon\,\eta(x), y^{*\prime}(x) + \epsilon \,\eta^{\prime}(x)\right)\,dx, \end{align*}\] <p>where, in turn, \(\eta(x)\) is a family of compactly supported, infinitely differentiable functions that are identically zero at the boundary \(x = a\) and \(x=b\). The key idea in variational Calculus is the reformulation of a functional optimization over \(y(x)\) into a variable one, over \(\epsilon\).</p> <p>Another key concept, the <em>fundamental lemma of Calculus of Variations</em> states that if a function \(f(x)\) satisfies the condition</p> \[\int_{\mathcal{X}} f(x)\,\eta(x)\,dx = 0\] <p>for all compactly supported, infinitely differentiable functions \(\eta(x)\), then \(f\) must be identically zero almost everywhere in \([a,b]\). Setting the first variation to zero and invoking the fundamental lemma of Calculus of Variations gives rise to the Euler-Lagrange condition.</p> <p>The Euler-Lagrange condition that the optimizer \(y^*(x)\) must satisfy is given as follows:</p> \[\frac{\partial\mathcal{F}}{\partial y} - \frac{\partial}{\partial x} \left(\frac{\partial\mathcal{F}}{\partial y^\prime} \right)\Bigg\vert_{y=y^*(x)}=0.\] <p>In the special case where the cost \(\mathcal{L}\) does not involve the derivative of \(y\), the EL condition reduces to the simplified form:</p> \[\frac{\partial\mathcal{F}}{\partial y}\Bigg|_{y=y^*(x)}=0,\] <p>which corresponds to a point-wise optimization with respect to \(y\), over the interval \([y(a),y(b)]\).</p> <h2 id="the-multivariate-case">The Multivariate Case</h2> <p>In the multivariate case, that is, \(\mathbf{x} \in \mathbb{R}^n\), the cost is of the type</p> \[\mathcal{L}\left(y\left(\mathbf{x}\right), \left\{y^{\prime}_{i}\right\}_{i=1}^{n} \right) =\int \limits_{\mathcal{X} \subseteq \mathbb{R}^n} \mathcal{F}\left(\mathbf{x}, y, \left\{y^{\prime}_{i} \right\}_{i=1}^{n}\right)\,d\mathbf{x},\] <p>where \(\mathcal{X}\) is the domain of integration and \(y^{\prime}_{i}\) denotes the partial derivative of \(y(\mathbf{x})\) w.r.t. the \(i^{\mathrm{th}}\) entry of \(\mathbf{x}\), that is, \(x_i\), and the boundary is defined as \(\partial\mathcal{X}\). The corresponding EL condition is</p> \[\frac{\partial \mathcal{F}}{\partial y} - \sum_{i=1}^{n} \left[ \frac{\partial}{\partial x_i} \left( \frac{\partial\mathcal{F}}{\partial y^{\prime}_{i}} \right)\right] \Bigg|_{y=y^*(\mathbf{x})}= 0.\] <p>The EL condition is a first-order condition and enforcing it yields the optimum. Whether the optimum corresponds to a minimizer or maximizer of the cost must be checked by invoking the second-order condition, more specifically the Legendre-Clebsch necessary condition for a minimizer. In the 1-D case, the condition is given by \(\frac{\partial^2 \mathcal{F}}{\partial y^{\prime 2}} \geq 0.\)</p> <table> <tbody> <tr> <td>In the multivariate setting, this condition translates to the positive-semi-definiteness (p.s.d.) of the Hessian matrix \(\mathbb{H}\) of the Hamiltonian \(\mathcal{H}\), computed with respect to \(\{y^{\prime}_i(\mathbf{x})\}_{i=1}^n\) and evaluated at \(y(\mathbf{x}) = y^*(\mathbf{x})\): $$\mathbb{H}_{y,\mathcal{H}}\big</td> <td>_{y=y^*} \succeq 0\(, where\)\succeq$$ denotes the p.s.d. property. The Hamiltonian is given by</td> </tr> </tbody> </table> \[\mathcal{H} = \sum_{i=1}^n \left( y_i^{\prime} \frac{\partial\mathcal{F}}{\partial y_i^{\prime}} \right) - \mathcal{F},\] <p>and the entries of the Hessian are given by</p> \[[\mathbb{H}_{y,\mathcal{H}}]_{i,j} = \frac{\partial^2 \mathcal{H}}{\partial y_i^{\prime}\partial y_j^{\prime}}.\] <p>As in the 1-D case, when the integral cost \(\mathcal{L}\) (and the integrand \(\mathcal{F}\)) do not depend on the derivatives \(\{y_i^{\prime}\}\), we have the simplified form of the EL condition:</p> \[\frac{\partial\mathcal{F}}{\partial y}\Bigg|_{y=y^*(\mathbf{x})}=0,\] <p>which is identical to the point-wise optimization seen in the scalar case. Similarly, when \(\frac{\partial\mathcal{F}}{\partial x} = 0\), we have the Beltrami identity \(\mathcal{F} - y^{\prime}\frac{\partial\mathcal{F}}{\partial y^{\prime}} = 0\).</p> <h2 id="the-brachistochrone-problem">The Brachistochrone Problem</h2> <p>The <strong>Brachistochrone</strong> or “shortest time” problem is considered to be one of the earliest known formulations of a variational problem, the solution to which supposedly gave rise to the field of Variational Calculus. The problem was posed by Johann Bernoulli in 1696 as follows: <em>“Given two points on a plane at different heights, what is the shape of the wire down which a bead will slide (without friction) under the influence of gravity so as to pass from the upper point to the lower point in the shortest amount of time?”</em> Isaac Newton and Jakob Bernoulli showed that the solution is a <em>cycloid</em>. The approach adopted by them was eventually formalized into the <em>Calculus of Variations</em> [1].</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/thesis/Brachistochrone/Problem.pdf" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/thesis/Brachistochrone/Solution.pdf" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Visualizations of the Brachistochrone problem. Given two points at different heights, we seek the curve that minimizes the travel time of a bead sliding under gravity without friction. The optimal solution to the Brachistochrone problem, the cycloid, results in the shortest travel time compared to other curves such as a straight line or polynomial path connecting the two points. </div> <h3 id="the-solution">The Solution</h3> <p>Given a curve \(y(x)\) joining the points \(\mathrm{A} = (0,a)\) and \(\mathrm{B} = (b,0)\), we can define the boundary conditions as \(y(0) = a\) and \(y(b) = 0\). The speed of a particle on the curve is given by \(v = \frac{ds}{dt} = \sqrt{\left(\frac{dx}{dt}\right)^2 + \left(\frac{dy}{dt}\right)^2}\), where \(s\) is the arc length. Then, we have \(ds = \sqrt{1 + (y^{\prime})^2}\,dx\). From the law of conservation of energy, under the action of gravity, we have \(v = \sqrt{2gy}\), where \(g\) is the acceleration due to gravity. Let the curve between \(\mathrm{A}\) and \(\mathrm{B}\) be of length \(s_y\), traversable in time \(t_y\). The functional optimization problem is given by:</p> \[y^*(x) = \arg\min_{y} \left\{ \int_{0}^{t_f} dt = \int_{0}^{s_y} \frac{1}{v}ds = \frac{1}{\sqrt{2g}}\int_{0}^{b} \sqrt{\frac{1 + (y^{\prime})^2}{y}} dx \right\}\] <p>Enforcing the Euler-Lagrange condition (and in particular, the Beltrami identity) yields:</p> \[\left(1 + (y^{\prime}(x))^2 \right) y(x) \big|_{y(x) = y^*(x)} = k,\] <p>for some constant \(k\). The above differential equation can be solved in polar coordinates, to result in the family of solutions</p> \[x = \frac{k}{2} \left( \theta - \sin(\theta)\right) \quad \text{and} \quad y = \frac{k}{2} \left( 1 - \cos(\theta)\right),\] <p>which are parametric equations of a cycloid.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/Brachistochrone.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls="" muted=""/> </figure> </div> </div> <div class="caption"> Animation of the Brachistochrone problem showing the cycloid path as the optimal solution for minimizing travel time under gravity. </div> <h2 id="the-higher-order-euler-lagrange-condition">The Higher-Order Euler-Lagrange Condition</h2> <p>We now recall the generalization to the EL condition, considering higher-order gradient penalties in \(n\)-D.</p> <p>Consider a vector \(\mathbf{x} = [x_1, x_2,\,\ldots\,,x_n]^{\mathsf{T}} \in \mathbb{R}^n\) and a function \(y:\mathbb{R}^n \rightarrow \mathbb{R}\). The notation \(\nabla_{\mathbf{x}}^m y(\mathbf{x})\) denotes the vector of \(m^{\mathrm{th}}\)-order partial derivatives of \(y\) with respect to the entries of \(\mathbf{x}\). We drop the subscript \(\mathbf{x}\) for convenience. \(\nabla^0\) is the identity operator. The elements of \(\nabla^m y\) are represented using the multi-index \(\boldsymbol{\alpha} = [\alpha_1, \alpha_2,\,\ldots\,,\alpha_n]^{\mathsf{T}}\), as:</p> \[\partial^{\boldsymbol{\alpha}} y = \frac{\partial^{|\boldsymbol{\alpha}|}}{\partial x_1^{\alpha_1}\partial x_2^{\alpha_2}\ldots \partial x_n^{\alpha_n} } y,\quad\text{where}\quad\boldsymbol{\alpha} \in \mathbb{Z}_{*}^n,\quad |\boldsymbol{\alpha}| = \sum_{i=1}^n \alpha_i,\] <p>where in turn \(\mathbb{Z}_{*}^n\) is the set of \(n\)-dimensional vectors with non-negative integer entries. For example, with \(n=4,m=3\), the index \(\boldsymbol{\alpha} = [2,0,0,1]^{\mathsf{T}}\) yields the element \(\frac{\partial^3}{\partial x_1^2 \partial x_4}y(\mathbf{x})\). Within this formulation, the higher-order gradient vector \(\nabla^m y\) can be operated on similar to the classical first-order gradient vector \(\nabla y\). For example, the square of the \(\ell_2\)-norm of \(\nabla^m y\) is given by a multidimensional sum:</p> \[\| \nabla^m y(\mathbf{x})\|_2^2 = \sum_{\boldsymbol{\alpha}:~|\boldsymbol{\alpha}| = m} \left( \frac{m!}{\boldsymbol{\alpha}!}\right) \left(\partial^{\boldsymbol{\alpha}} y(\mathbf{x})\right)^2,\] <p>where \(\boldsymbol{\alpha}! = \alpha_1!\alpha_2!\,\ldots\,\alpha_n!\).</p> <h3 id="higher-order-el-condition">Higher-Order EL Condition</h3> <p>Consider an integral cost \(\mathcal{L}\) with the integrand \(\mathcal{F}\) dependent on \(y\) and all its partial derivatives up to and including order \(\ell\), given by</p> \[\mathcal{L}\left(y(\mathbf{x}),\partial^{\boldsymbol{\alpha}} y;|\boldsymbol{\alpha}|\leq \ell \right) = \int_{\mathcal{X}} \mathcal{F} \left( y(\mathbf{x}),\partial^{\boldsymbol{\alpha}} y;|\boldsymbol{\alpha}|\leq \ell \right) d\mathbf{x},\] <p>defined on a suitable domain \(\mathcal{X}\) over which \(y\) and its partial derivatives up to and including order \(\ell\) are continuously differentiable.</p> <p>Consider the perturbations \(y(\mathbf{x})= y^*(\mathbf{x}) + \epsilon \eta(\mathbf{x})\), characterized by \(\eta(\mathbf{x})\) drawn from the family of compactly supported and infinitely differentiable functions. Then, the second-order Taylor-series approximation of \(g(\epsilon) = \mathcal{L}\left(y(\mathbf{x})\right) = \mathcal{L}\left(y^*(\mathbf{x}) + \epsilon \eta(\mathbf{x})\right)\) is given by</p> \[\begin{align*} g(\epsilon) &amp;= \mathcal{L}\left(y^*(\mathbf{x}) + \epsilon \eta(\mathbf{x})\right) \\ &amp;= \mathcal{L}\left(y^*(\mathbf{x})\right) + \epsilon\,\delta\mathcal{L}(y^*;\eta) + \frac{1}{2} \epsilon^2\, \delta^2\mathcal{L}(y^*;\eta), \end{align*}\] <p>where \(\delta\mathcal{L}(\cdot)\) and \(\delta^2\mathcal{L}(\cdot)\) denote the first variation and second variation of \(\mathcal{L}\), respectively, and can be evaluated through the scalar optimization problems [2]:</p> \[\delta\mathcal{L}(y^*(\mathbf{x})) = g^{\prime}(0) = \frac{\partial g(\epsilon)}{\partial\epsilon} \bigg|_{\epsilon = 0},\] <p>and</p> \[\delta^2\mathcal{L}(y^*(\mathbf{x})) = g^{\prime\prime}(0) = \frac{\partial^2 g(\epsilon)}{\partial\epsilon^2} \bigg|_{\epsilon = 0}.\] <p>Evaluating the first variation \(\delta\mathcal{L}(y^*;\eta) = \frac{\partial\mathcal{L}_{y,\epsilon}(\epsilon)}{\partial\epsilon}\) at \(\epsilon=0\) yields the Euler-Lagrange condition that the optimizer \(y^*\) must satisfy:</p> \[\left. \frac{\partial \mathcal{F}}{\partial y} + \sum_{j = 1}^{\ell} \left( (-1)^j \sum_{ \boldsymbol{\alpha}:\,|\boldsymbol{\alpha}| = j} \partial^{\boldsymbol{\alpha}} \left( \frac{\partial \mathcal{F}}{\partial ( \partial^{\boldsymbol{\alpha}} y )}\right) \right) \right|_{y= y^*} = 0.\] <p>As before, the EL condition merely provides us the extrema, and the second-order conditions must be checked to ascertain if the extrema is a minimizer or maximizer. The second-order Legendre condition for \(y^*(\mathbf{x})\) to be a minimizer is \(g^{\prime\prime}(0) &gt; 0\) [2].</p> <p>This general higher-order form reduces to the standard first-order condition when \(\ell=1\), and allows us to handle regularized optimization problems involving higher-order derivatives, which will be relevant in the analysis of gradient-penalized GANs and regularized discriminators.</p> <h2 id="connection-to-generative-adversarial-networks">Connection to Generative Adversarial Networks</h2> <p>The Euler-Lagrange framework provides a powerful mathematical foundation for analyzing GANs from a variational perspective. In the context of GANs:</p> <ul> <li>The <strong>discriminator optimization</strong> can be viewed as a functional optimization problem where we seek the optimal function \(D^*(\mathbf{x})\) that maximizes a divergence or IPM between distributions. <em>Regularized GANs</em> that incorporate gradient penalties (such as WGAN-GP [3], Sobolev GAN [4]) naturally fit into the higher-order Euler-Lagrange framework, where the regularizers constrain the class of admissible discriminator functions. The simplified EL condition (when derivatives are not involved) can be used to analyze unregularized GAN formulations, while the full EL condition with gradient terms will be essential for understanding regularized GANs. This variational perspective allows us to: <ol> <li>Derive closed-form expressions for optimal discriminators</li> <li>Analyze the effect of various regularization schemes</li> <li>Connect GAN optimization to classical problems in signal processing and interpolation theory</li> </ol> </li> <li>The <strong>generator optimization</strong> seeks the optimal transformation \(G^*(\mathbf{z})\) that minimizes a loss given the optimal discriminator. What will also be key in this setting, in the time-evolving nature of the generator, as the discriminator changes between iterations. We can further also use the EL setting to understand how GANs link to flow- and score-based diffusion models!!</li> </ul> <h3 id="navigation">Navigation</h3> <ul> <li><strong>Previous:</strong> <a href="/thesis-chapters/2023/05/10/thesis-chapter-1p3-FlowsDiffusion.html">Chapter 1.3: Flow-based and Diffusion Models</a></li> <li><strong>Next:</strong> <a href="/blog/2023/thesis-chapter-2-DivergenceGANs/">Chapter 2: Divergence-Minimizing GANs</a></li> <li><strong>Back to:</strong> <a href="/projects/1_thesis/">Thesis Project</a></li> </ul> <hr/> <h2 id="references">References</h2> <ol> <li> <p><strong>Goldstine, H. H. (1980).</strong> <em>A History of the Calculus of Variations from the 17th through the 19th Century</em>. Springer-Verlag.</p> </li> <li> <p><strong>Gelfand, I. M., &amp; Fomin, S. V. (1963).</strong> <em>Calculus of Variations</em>. Prentice-Hall.</p> </li> <li> <p><strong>Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., &amp; Courville, A. (2017).</strong> Improved training of Wasserstein GANs. In <em>Advances in Neural Information Processing Systems 30 (NIPS 2017)</em> (pp. 5767-5777).</p> </li> <li> <p><strong>Mroueh, Y., Sercu, T., &amp; Goel, V. (2018).</strong> Sobolev GAN. In <em>International Conference on Learning Representations (ICLR)</em>.</p> </li> <li> <p><strong>Jost, J., &amp; Li-Jost, X. (1998).</strong> <em>Calculus of Variations</em>. Cambridge University Press.</p> </li> <li> <p><strong>Dacorogna, B. (2007).</strong> <em>Introduction to the Calculus of Variations</em> (2nd ed.). Imperial College Press.</p> </li> <li> <p><strong>Evans, L. C. (2010).</strong> <em>Partial Differential Equations</em> (2nd ed.). American Mathematical Society.</p> </li> </ol>]]></content><author><name></name></author><category term="thesis-chapters-p0"/><category term="thesis"/><category term="calculus-of-variations"/><category term="euler-lagrange"/><category term="optimization"/><summary type="html"><![CDATA[An introduction to the Calculus of Variations and the Euler-Lagrange framework]]></summary></entry><entry><title type="html">Thesis Chapter 1.3: Flow-based Models and Diffusion</title><link href="https://www.siddarthasokan.com/thesis-chapters-p0/2023/05/10/thesis-chapter-1p3-FlowsDiffusion.html" rel="alternate" type="text/html" title="Thesis Chapter 1.3: Flow-based Models and Diffusion"/><published>2023-05-10T02:00:00+00:00</published><updated>2023-05-10T02:00:00+00:00</updated><id>https://www.siddarthasokan.com/thesis-chapters-p0/2023/05/10/thesis-chapter-1p3-FlowsDiffusion</id><content type="html" xml:base="https://www.siddarthasokan.com/thesis-chapters-p0/2023/05/10/thesis-chapter-1p3-FlowsDiffusion.html"><![CDATA[<blockquote> <p><em>What I cannot create, I do not understand.</em><br/> — Richard P. Feynman</p> </blockquote> <h2 id="flow-based-models">Flow-based Models</h2> <p>Flow-based models such as normalizing flows (NFs) [1] leverage the <em>change-of-variables</em> formula to learn a transformation from a parametric prior distribution to the target. Unlike in GANs and AEs, in NF models, the input to model \(\mathbf{z}\) is assumed to be of the same dimensionality as the output, <em>i.e.</em>, \(\mathbf{z}\in\mathbb{R}^{n}\). Normalizing flows model a forward process as the gradual <em>corruption</em> of a target distribution \(p_d\) to the noise distribution \(p_z\). The forward process is modeled as a series of <em>invertible</em> transformations \(f_i\), such that the composite function yields the desired <em>push-forward</em> distribution:</p> \[\mathbf{z} = \left( f_N \circ f_{N-1} \circ \cdots \circ f_2 \circ f_1\right) (\mathbf{x}) = f(\mathbf{x});~\mathbf{x}\sim p_d.\] <p>Then, by the <em>change-of-variables</em> formula, we have</p> \[p_z\left(\mathbf{z}\right) = p_d\left(f^{-1}(\mathbf{z})\right) \left\vert \mathrm{det}\, \mathbf{J}_{f^{-1}}(\mathbf{z})\right\vert = p_d\left(f^{-1}(\mathbf{z})\right) \left\vert \mathrm{det}\, \mathbf{J}_{f}\left(f^{-1}(\mathbf{z})\right)\right\vert^{-1},\] <p>where \(\left\vert \mathrm{det}\, \mathbf{J}_{f}(\cdot)\right\vert\) denotes the determinant of the Jacobian of \(f\). Consider a vector \(\mathbf{x} = [x_1, x_2,\,\ldots\,,x_n]^{\mathsf{T}} \in \mathbb{R}^n\) and the function \(f:\mathbb{R}^n \rightarrow \mathbb{R}^n\), <em>i.e.</em>, \(f(\mathbf{x}) = [f^1(\mathbf{x}), f^2(\mathbf{x}),\ldots,f^n(\mathbf{x})]\). The notation \(\nabla_{\mathbf{x}} f(\mathbf{x})\) represents the gradient matrix associated with the generator, with entries consisting of the partial derivatives of the entries of \(f\) with respect to the entries of \(\mathbf{x}\):</p> \[\nabla_{\mathbf{x}} f(\mathbf{x}) = \left[\begin{matrix} \frac{\partial f^1}{\partial x_1} &amp; \frac{\partial f^2}{\partial x_1} &amp; \ldots &amp; \ldots &amp;\frac{\partial f^n}{\partial x_1} \\[3pt] \frac{\partial f^1}{\partial x_2} &amp; \frac{\partial f^2}{\partial x_2} &amp; \ldots &amp; \ldots &amp;\frac{\partial f^n}{\partial x_2} \\[3pt] \vdots&amp;\vdots&amp;\ddots&amp;\ddots&amp;\vdots\\ \frac{\partial f^1}{\partial x_n} &amp; \frac{\partial f^2}{\partial x_n} &amp; \ldots &amp; \ldots &amp;\frac{\partial f^n}{\partial x_n} \end{matrix} \right].\] <p>The Jacobian \(\mathbf{J}\) can be thought of as <em>measuring</em> the transformation that the function imposes locally at the point of evaluation, and is defined to be the transpose of the gradient, <em>i.e.,</em> \(\nabla_{\mathbf{z}}f(\mathbf{z}) = \mathbf{J}_f^{\mathsf{T}}(\mathbf{z})\). The generative model is then defined as the <em>reverse</em> process, given by:</p> \[p_d(\mathbf{x}) = p_z\left(f^{-1}(\mathbf{x})\right) \left\vert \mathrm{det}\, \mathbf{J}_{f}(\mathbf{x})\right\vert = p_z\left(f^{-1}(\mathbf{x})\right) \prod_{i=1}^{N} \left\vert \mathrm{det}\, \mathbf{J}_{f_i}(\mathbf{z}_{i-1})\right\vert,\] <p>where \(\mathbf{z}_i = f_i(\mathbf{z}_{i-1})\) with \(\mathbf{z}_0 = \mathbf{z}\) and \(\mathbf{z}_N = \mathbf{x}\). An in-depth analysis of normalizing flows is presented by recent comprehensive surveys [2]. The \(f_i\) network architecture is constrained to facilitate easy computation of the Jacobian. Recent approaches design flows based on autoregressive models [3,4,5,6,7], or architectures motivated by the Sobolev GAN loss [9,10]. Flows have also been explored in the context of improving the quality of the noise input in GANs [8,11,12].</p> <h2 id="score-based-diffusion-models">Score-based Diffusion Models</h2> <p>Diffusion models are a recent class of generative models that implement discretized Langevin flow [13], and can be viewed as a random walk in a high-dimensional space. The random walks correspond to a Markov chain, whose stationary distribution converges to the desired distribution. As in the case of flow-based approaches, a forward process is assumed, wherein the data is <em>corrupted</em> with noise, while the generative model is an approximation of the reverse process. Langevin Monte Carlo (LMC) is the canonical algorithm for sampling from a given distribution in this framework, where we have access to the <em>score function</em>, \(\nabla_{\mathbf{x}}\ln p_d(\mathbf{x})\), which is the gradient of the logarithm of the target distribution. LMC is an instance of Markov chain Monte Carlo (MCMC) [16], and is a discretization of the Langevin diffusion stochastic differential equation given by</p> \[d\mathbf{x}(t) = -\nabla_{\mathbf{x}}f(\mathbf{x}(t))\,dt + \sqrt{2}\,d\mathbf{B}(t),\] <p>where \(\mathbf{B}(t)\) is the standard Brownian motion. The associated discretized update is given by</p> \[\mathbf{x}_{t+1} = \mathbf{x}_{t} - \epsilon \left. \nabla_{\mathbf{x}}f(\mathbf{x}) \right|_{\mathbf{x}=\mathbf{x}_t} + \sqrt{2\epsilon}\,\mathbf{z}_t,\] <p>where \(\mathbf{z}_{t}\) is an instance of the noise distribution drawn at time instant \(t\) and is independent of the sample \(\mathbf{x}_{t}\) generated at the corresponding time instant. As in normalizing flows, we have \(\mathbf{z}\in\mathbb{R}^{n}\). The evolution is initialized with parametric noise, <em>i.e.</em>, \(\mathbf{x}_0 \sim p_z\). The choice of the mapping function \(f\) gives rise to various diffusion models. For example, the popular family of score-based models [14,15] consider \(f(\mathbf{x}) = \nabla_{\mathbf{x}}\ln p_d(\mathbf{x})\), or in inverse-heat diffusion models [24], we have a frequency-domain transformation corresponding to Gaussian deblurring. In this thesis, we primarily focus on score-based diffusion, and their relation to GAN optimization.</p> <p>Existing score-based approaches train a neural network \(s_{\theta}(\mathbf{x})\) to approximate the score, by means of a score-matching loss [17], originally considered in the context of independent component analysis:</p> \[\mathcal{L}(\theta)=\frac{1}{2} \,\mathbb{E}_\sim p_d} \left[ \left\Vert s_{\theta}(\mathbf{x}) - \nabla_{\mathbf{x}}\ln p_d(\mathbf{x}) \right\Vert \right\Vert_2^2 \right].\] <p>The output of the trained network is used to generate samples through the annealed Langevin dynamics in noise-conditioned score networks (NCSN) [14]. However, a major limitation is that the predicted score is <em>weak</em> in regions far away from the target distribution, which is typically the case at the start of the Markov chain, around \(\mathbf{x}_0\sim p_z\). Various approaches such as noise scaling [14], sliced SM [18], and denoising SM [13,15] have been proposed to improve the strength of the gradients. Works considering improved discretization of the underlying differential equation [15,20,21,22] have been developed to accelerate the sampling process. Recently, denoising diffusion GANs (DDGANs) [23] were introduced, wherein a GAN is trained to model the diffusion process, with the generator and discriminator networks conditioned on the time index.</p> <h3 id="navigation">Navigation</h3> <ul> <li><strong>Previous:</strong> <a href="/thesis-chapters/2023/05/10/thesis-chapter-1p2-IntroGANs.html">Chapter 1.2: Generative Adversarial Networks</a></li> <li><strong>Next:</strong> <a href="/blog/2023/thesis-chapter-1p4-VariationalCalculus/">Chapter 1.4: An Introduction to Variational Calculus</a></li> <li><strong>Back to:</strong> <a href="/projects/1_thesis/">Thesis Project</a></li> </ul> <hr/> <h2 id="references">References</h2> <ol> <li> <p><strong>Rezende, D., &amp; Mohamed, S. (2015).</strong> Variational inference with normalizing flows. In <em>International Conference on Machine Learning (ICML)</em> (pp. 1530-1538).</p> </li> <li> <p><strong>Papamakarios, G., Nalisnick, E., Rezende, D. J., Mohamed, S., &amp; Lakshminarayanan, B. (2021).</strong> Normalizing flows for probabilistic modeling and inference. <em>Journal of Machine Learning Research</em>, 22(57), 1-64.</p> </li> <li> <p><strong>Dinh, L., Krueger, D., &amp; Bengio, Y. (2015).</strong> NICE: Non-linear independent components estimation. In <em>International Conference on Learning Representations (ICLR) Workshop</em>.</p> </li> <li> <p><strong>Dinh, L., Sohl-Dickstein, J., &amp; Bengio, S. (2017).</strong> Density estimation using Real NVP. In <em>International Conference on Learning Representations (ICLR)</em>.</p> </li> <li> <p><strong>Kingma, D. P., &amp; Dhariwal, P. (2018).</strong> Glow: Generative flow using invertible 1×1 convolutions. In <em>Advances in Neural Information Processing Systems 31 (NeurIPS 2018)</em> (pp. 10215-10224).</p> </li> <li> <p><strong>Kingma, D. P., Salimans, T., Jozefowicz, R., Chen, X., Sutskever, I., &amp; Welling, M. (2016).</strong> Improved variational inference with inverse autoregressive flow. In <em>Advances in Neural Information Processing Systems 29 (NIPS 2016)</em> (pp. 4743-4751).</p> </li> <li> <p><strong>Papamakarios, G., Pavlakou, T., &amp; Murray, I. (2017).</strong> Masked autoregressive flow for density estimation. In <em>Advances in Neural Information Processing Systems 30 (NIPS 2017)</em> (pp. 2338-2347).</p> </li> <li> <p><strong>Chen, X., Kingma, D. P., Salimans, T., Duan, Y., Dhariwal, P., Schulman, J., Sutskever, I., &amp; Abbeel, P. (2018).</strong> Variational lossy autoencoder. In <em>International Conference on Learning Representations (ICLR)</em>.</p> </li> <li> <p><strong>Mroueh, Y., Sercu, T., &amp; Goel, V. (2019).</strong> Sobolev descent. In <em>International Conference on Artificial Intelligence and Statistics (AISTATS)</em> (pp. 2976-2984).</p> </li> <li> <p><strong>Gong, C., Wang, D., &amp; Liu, Q. (2020).</strong> Unbiased Sobolev descent. In <em>International Conference on Machine Learning (ICML)</em> (pp. 3558-3567).</p> </li> <li> <p><strong>Kumar, M., Weissenborn, D., &amp; Kalchbrenner, N. (2021).</strong> Regularizing normalizing flows with Kallenberg-Leibler divergence. In <em>Conference on Uncertainty in Artificial Intelligence (UAI)</em>.</p> </li> <li> <p><strong>Dinh, L., Sohl-Dickstein, J., Pascanu, R., &amp; Larochelle, H. (2021).</strong> A RAD approach to deep mixture models. In <em>International Conference on Learning Representations (ICLR)</em>.</p> </li> <li> <p><strong>Ho, J., Jain, A., &amp; Abbeel, P. (2020).</strong> Denoising diffusion probabilistic models. In <em>Advances in Neural Information Processing Systems 33 (NeurIPS 2020)</em> (pp. 6840-6851).</p> </li> <li> <p><strong>Song, Y., &amp; Ermon, S. (2019).</strong> Generative modeling by estimating gradients of the data distribution. In <em>Advances in Neural Information Processing Systems 32 (NeurIPS 2019)</em> (pp. 11895-11907).</p> </li> <li> <p><strong>Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., &amp; Poole, B. (2021).</strong> Score-based generative modeling through stochastic differential equations. In <em>International Conference on Learning Representations (ICLR)</em>.</p> </li> <li> <p><strong>Sinclair, A. (1992).</strong> Improved bounds for mixing rates of Markov chains and multicommodity flow. <em>Combinatorics, Probability and Computing</em>, 1(4), 351-370.</p> </li> <li> <p><strong>Hyvärinen, A. (2005).</strong> Estimation of non-normalized statistical models by score matching. <em>Journal of Machine Learning Research</em>, 6, 695-709.</p> </li> <li> <p><strong>Song, Y., &amp; Ermon, S. (2020).</strong> Improved techniques for training score-based generative models. In <em>Advances in Neural Information Processing Systems 33 (NeurIPS 2020)</em> (pp. 12438-12448).</p> </li> <li> <p><strong>Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., &amp; Poole, B. (2021).</strong> Score-based generative modeling through stochastic differential equations. In <em>International Conference on Learning Representations (ICLR)</em>.</p> </li> <li> <p><strong>Song, Y., Durkan, C., Murray, I., &amp; Ermon, S. (2020).</strong> Maximum likelihood training of score-based diffusion models. In <em>Advances in Neural Information Processing Systems 33 (NeurIPS 2020)</em> (pp. 1415-1428).</p> </li> <li> <p><strong>Jolicoeur-Martineau, A., Li, K., Piché-Taillefer, R., Kachman, T., &amp; Mitliagkas, I. (2021).</strong> Gotta go fast when generating data with score-based models. <em>arXiv preprint arXiv:2105.14080</em>.</p> </li> <li> <p><strong>Karras, T., Aittala, M., Aila, T., &amp; Laine, S. (2022).</strong> Elucidating the design space of diffusion-based generative models. In <em>Advances in Neural Information Processing Systems 35 (NeurIPS 2022)</em> (pp. 26565-26577).</p> </li> <li> <p><strong>Xiao, Z., Kreis, K., &amp; Vahdat, A. (2022).</strong> Tackling the generative learning trilemma with denoising diffusion GANs. In <em>International Conference on Learning Representations (ICLR)</em>.</p> </li> <li> <p><strong>Bansal, A., Borgnia, E., Chu, H.-M., Li, J., Kazemi, H., Huang, F., Goldblum, M., Geiping, J., &amp; Goldstein, T. (2023).</strong> Cold diffusion: Inverting arbitrary image transforms without noise. In <em>Advances in Neural Information Processing Systems 36 (NeurIPS 2023)</em>.</p> </li> </ol>]]></content><author><name></name></author><category term="thesis-chapters-p0"/><category term="thesis"/><category term="flows"/><category term="diffusion"/><category term="machine-learning"/><category term="generative-models"/><summary type="html"><![CDATA[An introduction to normalizing flows and score-based diffusion models]]></summary></entry><entry><title type="html">Thesis Chapter 1.2: Introduction to Generative Adversarial Networks</title><link href="https://www.siddarthasokan.com/thesis-chapters-p0/2023/05/10/thesis-chapter-1p2-introGANs.html" rel="alternate" type="text/html" title="Thesis Chapter 1.2: Introduction to Generative Adversarial Networks"/><published>2023-05-10T01:00:00+00:00</published><updated>2023-05-10T01:00:00+00:00</updated><id>https://www.siddarthasokan.com/thesis-chapters-p0/2023/05/10/thesis-chapter-1p2-introGANs</id><content type="html" xml:base="https://www.siddarthasokan.com/thesis-chapters-p0/2023/05/10/thesis-chapter-1p2-introGANs.html"><![CDATA[<blockquote> <p><em>What I cannot create, I do not understand.</em><br/> — Richard P. Feynman</p> </blockquote> <h2 id="generative-adversarial-networks-gans">Generative Adversarial Networks (GANs)</h2> <p>The original GAN formulation presented by Goodfellow et al. (2014) [1], (hereafter referred to as the Standard GAN, or SGAN) is a <em>min-max</em> game between two players — a generator (\(G\)), and a discriminator (\(D\)), both of which are typically implemented as neural networks. The role of the generator, akin to the decoder in AEs and VAEs, is to create fake samples that mimic the ones coming from the training data distribution. However, the generated samples are not trained with a one-to-one correspondence with the real/target data samples. Instead, the generator is trained to create samples that appear realistic to a <em>critic</em>, which is the discriminator network \(D\) tasked with telling apart the real samples from the fake ones. The optimal \(G\) is the one that <em>outsmarts</em> \(D\) into confusing the fake samples for real.</p> <p>We briefly introduce the GAN optimization framework here. An in-depth discussion of various GAN losses and their regularization schemes is presented in Section 1.3, while a comprehensive study of GAN algorithms and training practices for image generation, video synthesis, and neural rendering is provided in recent surveys [2]. The SGAN optimization comprises the generator and the discriminator with respective loss functions. The generator \(G\) accepts high-dimensional noise \(\mathbf{z} \sim p_z\) as input and generates fake samples \(\mathbf{x} = G(\mathbf{z}) \sim p_g\). Unlike in VAEs, the latent space in GANs is not learnt, but assumed to be a standard parametric distribution, typically Gaussian or uniformly distributed in \(\mathbb{R}^{100}\) or \(\mathbb{R}^{128}\). The discriminator \(D\) accepts an input \(\mathbf{x}\), which could come from either the data distribution \(p_d\) as a sample from the dataset \(\mathcal{D}\), or the generator distribution \(p_g\) as the output of the generator. The discriminator outputs a value \(D(\mathbf{x})\), which serves as a measure of <em>how realistic</em> the input sample \(\mathbf{x}\) is. Effectively, the generator must learn a mapping from the noise distribution to the data distribution, whereas the discriminator must learn the optimal two-class classifier.</p> <p>The SGAN discriminator \(D_{\phi}\) and generator \(G_{\theta}\) are implemented as neural networks and their parameters \((\phi,\theta)\) are optimized as follows:</p> \[\min_{\theta} \left\{ \max_{\phi} \Big\{ \mathbb{E}_{\mathbf{x}\sim p_d} \left[\ln\left(D_{\phi}(\mathbf{x})\right)\right] + \mathbb{E}_{\mathbf{z}\sim p_z} \left[ \ln\left(1 - D_{\phi}(G_{\theta}(\mathbf{z}))\right) \right] \Big\} \right\}.\] <p>Ideally, the min-max problem can be solved simultaneously to attain the <em>Nash equilibrium</em> of the two-player game. In practice, however, alternating gradient ascent-descent algorithms are used to train the networks until a suitable convergence criterion is met.</p> <p>Over the past decade, numerous variants of GANs have been proposed with several successful applications. From an analytical standpoint, the discriminator in these variants can be viewed as approximating a <em>measure of dissimilarity</em> between \(p_d\) and \(p_g\). On the other hand, the generator can be viewed as minimizing the measure that the discriminator approximates. Almost all known GAN flavors consider either a divergence or an integral probability metric. We review important GANs under each category.</p> <h3 id="divergence-minimizing-gans">Divergence-minimizing GANs</h3> <p>GANs were originally designed to minimize the divergence between the <em>target</em> data distribution \(p_d\) and the generator distribution \(p_g\). For instance, the SGAN formulation corresponds to optimizing the Jensen-Shannon divergence, whereas the least-squares GAN (LSGAN) [3] has been shown to optimize the Pearson-\(\chi^2\) divergence under certain assumptions. The \(f\)-GAN formulation [4] is a generalization to any choice of \(f\)-divergence, while other works [5] proposed extensions to various Bregman divergences. In the original SGAN, the discriminator output is constrained to the interval \([0,1]\), representing the probability of the input sample being real or fake, whereas LSGAN requires the discriminator output to match a set of chosen class-labels. In \(f\)-GANs, the discriminator output is real-valued, but an appropriately chosen activation function maps it to a desired interval.</p> <h3 id="integral-probability-metric-ipm-gans">Integral Probability Metric (IPM) GANs</h3> <p>The divergence metric approaches fail if \(p_d\) and \(p_g\) are of disjoint support [6], which shifted focus to <em>integral probability metrics</em> (IPMs). In these GAN flavors, the <em>discriminator</em> is replaced with a <em>real-valued critic</em> \(C\) that differentiates between the generator and data distributions in terms of an integral probability metric (IPM) defined over the class of critics to choose from. The choice of the class of critics gives rise to variants such as the Wasserstein GAN (WGAN) [7] with a Lipschitz-1 critic, the Fisher GAN [8] in which the second-order moments of the critic are bounded or Sobolev GANs [9], which favor critics with a finite energy in the gradient. The critic is a neural network similar to the discriminator, where the constraints are enforced appropriately, either by means of an adjustment of the network weights [7,10,11], or through a suitable penalty incorporated into the loss function [12,13,14]. Throughout this thesis, we use the term <em>discriminator</em> \(D(\mathbf{x})\) to refer to either a divergence-based discriminator or the IPM-based critic with the context and GAN loss function resolving any ambiguity.</p> <h2 id="wasserstein-autoencoders">Wasserstein Autoencoders</h2> <p>Wasserstein autoencoders (WAEs) [15] and adversarial autoencoders (AAE) [16] lie at the intersection of AEs and GANs. In these models, a vanilla autoencoder’s [17,18] latent space representation is required to conform to a given <em>prior</em> distribution, usually a Gaussian or a mixture of Gaussians, through an auxiliary network that minimizes the distance between the two distributions. While VAEs achieve this by means of a loss defined via an evidence lower bound, in AAEs and WAEs, the GAN formulation is leveraged.</p> <p>The <em>encoder-decoder</em> pair is trained by minimizing an appropriate distance measured between the input data distribution and that of the reconstructed samples. In the WAE-GAN setting, the encoder of the WAE also plays the role of the GAN generator, which is trained using a discriminator network to force the latent space distribution to match the prior using a suitable GAN loss. Considering the Euclidean distance metric between a data sample \(\mathbf{x}\) and the corresponding reconstruction \(\tilde{\mathbf{x}}\), and the SGAN loss, we obtain the AAE formulation. The vanilla WAE-GAN formulation employs the Euclidean loss for the reconstruction in combination with the KL divergence for the GAN loss. Sliced WAE [19] extended the framework to accommodate the sliced Wasserstein loss [20]. As an alternative to the adversarial formulation, maximum mean discrepancy (MMD) based variations of the WAE have also gained popularity, the most recent of which, the Cramér-Wold autoencoder (CWAE) [21] presents a characteristic kernel that allows for closed-form computation of the distance between the latent distribution and a standard Gaussian. Optimal transport based approaches either approximate the semi-discrete latent-space transportation map with the continuous Bernier potential by drawing links between the latent-space matching in WAEs and the Monge-Ampère partial differential equation [22,23] or determine the Kantorovich potential in a two-step manner to learn a mapping from the source to the target using a WGAN-GP discriminator [24]. Adding regularizers to the discriminator cost in AAEs has been shown to improve the interpolation capabilities of the AE [25].</p> <h2 id="a-deep-dive-into-gans">A Deep Dive into GANs</h2> <p>Divergence-minimizing GANs consider a discriminator loss that approximates a divergence between \(p_d\) and \(p_g\). Table 1 presents the discriminator and generator loss functions of various divergence-minimizing GANs. The standard GAN (SGAN) proposed by Goodfellow et al. (2014) [1] considers both saturating and non-saturating losses. The term saturation refers to the generator gradients vanishing during training. The vanilla SGAN (employing the saturating loss) results in a min-max zero-sum game where the optimal generator minimizes the Jensen-Shannon divergence between \(p_d\) and \(p_g\). On the contrary, the SGAN with a non-saturating loss (SGAN-NS) does not readily map to a divergence, but results in a superior performance when training the generator in practice. In the least-squares GAN (LSGAN) [3], one minimizes the squared distance between the discriminator prediction and the class labels \((a,b,c)\) for fake, real, and generated samples, respectively. The loss is not symmetric – the discriminator is trained to assign a class label \(a\) to the <em>fakes</em> and a class label \(b\) to the <em>reals</em>, while the generator loss is designed to promote <em>confusion</em> by assigning a label \(c\) to both the real and fake samples. The optimization objective can be seen as minimizing the Pearson-\(\chi^2\) divergence when \(b-c=1\) and \(b-a=2\).</p> <p>Nowozin et al. (2016) [4] considered \(f\)-divergences of the form:</p> \[\mathfrak{D}_f(p_d\|p_g) = \int_\mathcal{X} p_g(\mathbf{x}) f\left(\frac{p_d(\mathbf{x})}{p_g(\mathbf{x})} \right)\,d\mathbf{x},\] <p>where the divergence function \(f: \mathbb{R}_+ \rightarrow \mathbb{R}\) is convex, lower-semicontinuous and satisfies \(f(1) = 0\), and \(\mathcal{X}\) is a suitable domain of integration. They demonstrated that, in a GAN setting, the minimization of \(\mathfrak{D}_f(p_g\|p_d)\) is equivalent to the sequential minimization of the discriminator and generator losses:</p> \[\mathcal{L}_D^{f} = -\mathbb{E}_{\mathbf{x} \sim p_d}[T(\mathbf{x})] + \mathbb{E}_{\mathbf{x} \sim p_g}[f^{c}(T(\mathbf{x}))]\text{, and}\] \[\mathcal{L}_G^{f}= \mathbb{E}_{\mathbf{x} \sim p_{d}}[T^*(\mathbf{x})]-\mathbb{E}_{\mathbf{x} \sim p_g}[f^{c}(T^*(\mathbf{x}))] \text{,}\] <p>with respect to \(D\) and \(p_g\), respectively, where \(T(\mathbf{x})\) is the output of the discriminator subjected to activation \(g\), that is, \(T(\mathbf{x}) = g(D(\mathbf{x}))\) and \(T^*(\mathbf{x}) = g(D^*(\mathbf{x}))\), where \(D^*(\mathbf{x})\) is the optimal discriminator, and \(f^{c}\) is the Fenchel conjugate of \(f\). The choice of the divergence \(f\) and the activation \(g\) gives rise to GANs that minimize divergences such as Kullback-Leibler (KL), reverse KL, Pearson-\(\chi^2\), squared Hellinger, Jensen-Shannon, etc.</p> <h3 id="table-1-divergence-minimizing-gan-losses">Table 1: Divergence-Minimizing GAN Losses</h3> <style>.table-wrapper table td:first-child{width:20%}</style> <table> <thead> <tr> <th>GAN Variant</th> <th>Discriminator Loss: \(\mathcal{L}_D\)</th> <th>Generator Loss: \(\mathcal{L}_G\)</th> </tr> </thead> <tbody> <tr> <td><strong>SGAN</strong></td> <td>\(\mathcal{L}_D^{\mathrm{S}} = -\mathbb{E}_{\mathbf{x} \sim p_d}[\ln D(\mathbf{x})]-\mathbb{E}_{\mathbf{x} \sim p_g}[\ln (1-D(\mathbf{x}))]\)</td> <td>\(\mathcal{L}_G^{\mathrm{S}} = \mathbb{E}_{\mathbf{x} \sim p_d}[\ln D^*(\mathbf{x})]+\mathbb{E}_{\mathbf{x} \sim p_g}[\ln (1-D^*(\mathbf{x}))]\)</td> </tr> <tr> <td><strong>SGAN-NS</strong></td> <td>\(\mathcal{L}_D^{\mathrm{NS}} = -\mathbb{E}_{\mathbf{x} \sim p_d}[\ln D(\mathbf{x})]-\mathbb{E}_{\mathbf{x} \sim p_g}[\ln (1-D(\mathbf{x}))]\)</td> <td>\(\mathcal{L}_G^{\mathrm{NS}} = -\mathbb{E}_{\mathbf{x} \sim p_d}[\ln (1-D^*(\mathbf{x}))]-\mathbb{E}_{\mathbf{x} \sim p_g}[\ln D^*(\mathbf{x})]\)</td> </tr> <tr> <td><strong>LSGAN</strong></td> <td>\(\mathcal{L}_D^{\mathrm{LS}} =\mathbb{E}_{\mathbf{x} \sim p_d}[(D(\mathbf{x})-b)^2]+\mathbb{E}_{\mathbf{x} \sim p_g}[(D(\mathbf{x})-a)^2]\)</td> <td>\(\mathcal{L}_G^{\mathrm{LS}} =\mathbb{E}_{\mathbf{x} \sim p_d}[(D^*(\mathbf{x})-c)^2]+\mathbb{E}_{\mathbf{x} \sim p_g}[(D^*(\mathbf{x})-c)^2]\)</td> </tr> <tr> <td><strong>\(f\)-GAN</strong></td> <td>\(\mathcal{L}_D^{f} =-\mathbb{E}_{\mathbf{x} \sim p_d}[g(D(\mathbf{x}))] + \mathbb{E}_{\mathbf{x} \sim p_g}[f^{c}(g(D(\mathbf{x})))]\)</td> <td>\(\mathcal{L}_G^{f} =\mathbb{E}_{\mathbf{x} \sim p_d}[g(D^*(\mathbf{x}))]-\mathbb{E}_{\mathbf{x} \sim p_g}[f^{c}(g(D^*(\mathbf{x})))]\)</td> </tr> </tbody> </table> <p><strong>Table 1:</strong> A summary of various divergence minimizing GAN losses. The SGAN and \(f\)-GAN losses are symmetric and lead to a min-max optimization problem, whereas the LSGAN and SGAN-NS are not symmetric. The LSGAN loss is parametrized by class labels \((a,b,c)\), wherein the discriminator learns a classifier with the class label \(b\) for the <em>reals</em> and a class label \(a\) for the <em>fakes</em>. The LSGAN generator is trained to <em>confuse</em> the discriminator by enforcing the same class label \(c\) to both the reals and the fakes. Various choices of the Csiszár divergence function \(f\) (and consequently, its conjugate \(f^c\)), and <em>activation</em> function \(g\) give rise to the family of \(f\)-GANs.</p> <h3 id="ipm-based-gans-and-regularization">IPM-Based GANs and Regularization</h3> <p>The divergence metric approaches fail if \(p_d\) and \(p_g\) are of disjoint support [6], which shifted focus to <em>integral probability metrics</em> (IPMs), where a <em>critic</em> function is chosen to approximate a chosen IPM between the distributions [7,8]. Choosing the IPM is equivalent to constraining the class of functions from which the critic/discriminator is drawn. The most popular variant, inspired by optimal transport, is the Wasserstein GAN (WGAN) [7], in which the objective is to minimize the Wasserstein-1 or <em>earth mover’s</em> distance between \(p_d\) and \(p_g\), and the critic is constrained to be Lipschitz-1. Gulrajani et al. (2017) [12] enforced a first-order gradient penalty on the discriminator network to approximate the Lipschitz constraint. Here, the loss is defined via the Kantorovich–Rubinstein duality, subjected to a regularizer \(\Omega_D\):</p> \[\mathcal{L}_D^W = \mathbb{E}_{\mathbf{x} \sim p_d}[D(\mathbf{x})]- \mathbb{E}_{\mathbf{x} \sim p_g}[D(\mathbf{x})] + \lambda_d\Omega_D,\] <p>where \(\lambda_d\) is the Lagrange multiplier associated with the regularizer. Table 2 summarizes the IPM-GAN losses, the corresponding constraint space of the discriminator, and the ideal form of the regularizer \(\Omega_D\). However, these regularizers are usually replaced with an implementable counterpart, typically involving first-order gradient-based terms [12,14,31,32]. On the other hand, several works [13,27,28,33] showed the empirical success of the first-order gradient penalty in divergence-minimizing GANs. Cramér GANs [34] and Sobolev GANs [9,29] bound the energy in the gradients of \(D\), enforcing Sobolev-space constraints.</p> <h3 id="table-2-ipm-gan-losses">Table 2: IPM-GAN Losses</h3> <style>.table-wrapper table td:first-child{width:20%}</style> <table> <thead> <tr> <th>GAN Variant</th> <th>Unregularized Discriminator Loss</th> <th>Closed-form Metric: \(\mathfrak{D}\)</th> <th>Discriminator Constraint Space</th> </tr> </thead> <tbody> <tr> <td><strong>\(f\)-GANs</strong></td> <td>\(-\mathbb{E}_{\mathbf{x} \sim p_d}[T(\mathbf{x})] +\mathbb{E}_{\mathbf{x} \sim p_g}[f^{c}(T(\mathbf{x}))]\)</td> <td>\(\mathbb{E}_{\mathbf{x}\sim p_g} \left[ f\left( \frac{p_d(\mathbf{x})}{p_g(\mathbf{x})}\right) \right]\)</td> <td>\(\{T = g(D(\cdot)):\,\mathcal{X} \rightarrow \mathbb{R}\,;\,T\in\mathrm{dom}(f)\}\)</td> </tr> <tr> <td><strong>WGAN</strong></td> <td>\(\mathbb{E}_{\mathbf{x} \sim p_d}[D(\mathbf{x})]- \mathbb{E}_{\mathbf{x} \sim p_g}[D(\mathbf{x})]\)</td> <td>—-</td> <td>\(\{D:\,\mathcal{X} \rightarrow \mathbb{R}\,;\,|D|_{\mathrm{Lip}}\leq 1 \}\)</td> </tr> <tr> <td><strong>\(\mu\)-Fisher-GAN</strong></td> <td>\(\mathbb{E}_{\mathbf{x} \sim p_d}[D(\mathbf{x})]- \mathbb{E}_{\mathbf{x} \sim p_g}[D(\mathbf{x})]\)</td> <td>\(\sqrt{ \mathbb{E}_{\mathbf{x}\sim\mu(\mathbf{x})} \left[\left(\frac{p_d(\mathbf{x}) - p_g(\mathbf{x})}{\mu(\mathbf{x})}\right)^2 \right]}\)</td> <td>\(\left\{D:\,\mathcal{X}\rightarrow \mathbb{R}\,;\,\mathbb{E}_{\mathbf{x}\sim\mu} \left[D^2(\mathbf{x})\right]\leq 1\right\}\)</td> </tr> <tr> <td><strong>Cramér-GAN</strong></td> <td>\(\mathbb{E}_{\mathbf{x} \sim p_d}[D(\mathbf{x})]- \mathbb{E}_{\mathbf{x} \sim p_g}[D(\mathbf{x})]\)</td> <td>\(\sqrt{ \mathbb{E}_{x\sim p_g} \left[\left(\frac{\mathrm{CDF}_{p_d}(x) - \mathrm{CDF}_{p_g}(x)}{p_d(x)}\right)^2 \right]}\)</td> <td>\(\left\{D:\,\mathcal{X}\rightarrow \mathbb{R}\,;\,\mathbb{E}_{x\sim p_d} \left[ \left(\frac{\partial D(x)}{\partial x}\right)^2\right]\leq 1\right\}\)</td> </tr> <tr> <td><strong>\(\mu\)-Sobolev-GAN</strong></td> <td>\(\mathbb{E}_{\mathbf{x} \sim p_d}[D(\mathbf{x})]- \mathbb{E}_{\mathbf{x} \sim p_g}[D(\mathbf{x})]\)</td> <td>\(\sqrt{ \mathbb{E}_{\mathbf{x}\sim\mu(\mathbf{x})} \left[ \sum_{i=1}^n\left(\frac{\mathrm{CDF}_{i,p_d}(\mathbf{x}) - \mathrm{CDF}_{i,p_g}(\mathbf{x})}{n\,\mu(\mathbf{x})}\right)^2 \right]}\)</td> <td>\(\left\{D:\,\mathcal{X}\rightarrow \mathbb{R}\,;\,\mathbb{E}_{\mathbf{x}\sim\mu} \left[ |\nabla_{\mathbf{x}} D(\mathbf{x})|_2^2\right]\leq 1\right\}\)</td> </tr> <tr> <td><strong>MMD-GANs</strong></td> <td>\(\mathbb{E}_{\mathbf{x} \sim p_d}[D(\mathbf{x})]- \mathbb{E}_{\mathbf{x} \sim p_g}[D(\mathbf{x})]\)</td> <td>\(\left| \mathbb{E}_{\mathbf{x}\sim p_g} \left[\phi_{k}(\mathbf{x})\right] - \mathbb{E}_{\mathbf{x}\sim p_d} \left[\phi_{k}(\mathbf{x})\right] \right|_{\mathscr{H}_{\phi_{k}}}\)</td> <td>\(\{D:\,\mathcal{X} \rightarrow \mathbb{R}\,;\,|D|_{\mathscr{H}_{\phi_{k}}} \leq 1\}\)</td> </tr> </tbody> </table> <p><strong>Table 2:</strong> A summary of various IPM-GAN losses considered in the literature, compared against an \(f\)-GAN loss. Various choices of the \(f\)-divergence function give rise to the family of \(f\)-GANs. Other GAN variants consider the standard Wasserstein-1 IPM, with varying choices for the discriminator regularizer (the <em>closed-form metric</em>), which in turn induces a constraint space for the discriminator. WGAN enforces a Lipschitz-1 discriminator by means of weight-clipping on the network, while Fisher GANs draw discriminators with finite \(L_2\) norms. While Cramér GANs enforce a gradient-norm penalty in 1-D with a metric defined over the cumulative density functions (CDF), Sobolev GANs consider a generalization involving the sum of all marginal CDFs (indexed by \(i\)). Maximum mean discrepancy GANs (MMD-GANs) derived a kernel-mean statistic associated with the RKHS \(\mathscr{H}_{\phi_{k}}\).</p> <h3 id="navigation">Navigation</h3> <ul> <li><strong>Previous:</strong> <a href="/thesis-chapters/2023/05/10/thesis-chapter-1p1-IntroGenMod.html">Chapter 1.1: Introduction to Generative Modeling</a></li> <li><strong>Next:</strong> <a href="/thesis-chapters/2023/05/10/thesis-chapter-1p3-FlowsDiffusion.html">Chapter 1.3: Flow-based and Diffusion Models</a></li> <li><strong>Back to:</strong> <a href="/projects/1_thesis/">Thesis Project</a></li> </ul> <hr/> <h2 id="references">References</h2> <ol> <li> <p><strong>Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., &amp; Bengio, Y. (2014).</strong> Generative adversarial nets. In <em>Advances in Neural Information Processing Systems 27 (NIPS 2014)</em> (pp. 2672-2680).</p> </li> <li> <p><strong>Liu, M.-Y., Huang, X., Yu, J., Wang, T.-C., &amp; Mallya, A. (2021).</strong> Generative adversarial networks for image and video synthesis: Algorithms and applications. <em>Proceedings of the IEEE</em>, 109(5), 839-862.</p> </li> <li> <p><strong>Mao, X., Li, Q., Xie, H., Lau, R. Y., Wang, Z., &amp; Smolley, S. P. (2017).</strong> Least squares generative adversarial networks. In <em>International Conference on Computer Vision (ICCV)</em> (pp. 2813-2821).</p> </li> <li> <p><strong>Nowozin, S., Cseke, B., &amp; Tomioka, R. (2016).</strong> f-GAN: Training generative neural samplers using variational divergence minimization. In <em>Advances in Neural Information Processing Systems 29 (NIPS 2016)</em> (pp. 271-279).</p> </li> <li> <p><strong>Tao, C., Chen, L., Henao, R., Feng, J., &amp; Carin, L. (2018).</strong> Chi-square generative adversarial network. In <em>International Conference on Machine Learning (ICML)</em> (pp. 4843-4852).</p> </li> <li> <p><strong>Arjovsky, M., &amp; Bottou, L. (2017).</strong> Towards principled methods for training generative adversarial networks. In <em>International Conference on Learning Representations (ICLR)</em>.</p> </li> <li> <p><strong>Arjovsky, M., Chintala, S., &amp; Bottou, L. (2017).</strong> Wasserstein generative adversarial networks. In <em>International Conference on Machine Learning (ICML)</em> (pp. 214-223).</p> </li> <li> <p><strong>Mroueh, Y., &amp; Sercu, T. (2017).</strong> Fisher GAN. In <em>Advances in Neural Information Processing Systems 30 (NIPS 2017)</em> (pp. 2513-2523).</p> </li> <li> <p><strong>Mroueh, Y., Sercu, T., &amp; Goel, V. (2018).</strong> Sobolev GAN. In <em>International Conference on Learning Representations (ICLR)</em>.</p> </li> <li> <p><strong>Li, Z., Hu, Z., Luo, S., Chen, S., Gao, K., Chen, C., Yang, J., &amp; Xu, B. (2019).</strong> Spectral normalization for generative adversarial networks. In <em>International Conference on Learning Representations (ICLR)</em>.</p> </li> <li> <p><strong>Wang, D., Gong, Z., &amp; Liu, Q. (2016).</strong> Stein variational gradient descent as gradient flow. In <em>Advances in Neural Information Processing Systems 29 (NIPS 2016)</em>.</p> </li> <li> <p><strong>Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., &amp; Courville, A. (2017).</strong> Improved training of Wasserstein GANs. In <em>Advances in Neural Information Processing Systems 30 (NIPS 2017)</em> (pp. 5767-5777).</p> </li> <li> <p><strong>Kodali, N., Abernethy, J., Hays, J., &amp; Kira, Z. (2017).</strong> On convergence and stability of GANs. <em>arXiv preprint arXiv:1705.07215</em>.</p> </li> <li> <p><strong>Mescheder, L., Geiger, A., &amp; Nowozin, S. (2018).</strong> Which training methods for GANs do actually converge? In <em>International Conference on Machine Learning (ICML)</em> (pp. 3481-3490).</p> </li> <li> <p><strong>Tolstikhin, I., Bousquet, O., Gelly, S., &amp; Schoelkopf, B. (2018).</strong> Wasserstein auto-encoders. In <em>International Conference on Learning Representations (ICLR)</em>.</p> </li> <li> <p><strong>Makhzani, A., Shlens, J., Jaitly, N., Goodfellow, I., &amp; Frey, B. (2015).</strong> Adversarial autoencoders. <em>arXiv preprint arXiv:1511.05644</em>.</p> </li> <li> <p><strong>Hinton, G. E., &amp; Zemel, R. S. (1994).</strong> Autoencoders, minimum description length and Helmholtz free energy. In <em>Advances in Neural Information Processing Systems 6 (NIPS 1993)</em> (pp. 3-10).</p> </li> <li> <p><strong>Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016).</strong> <em>Deep Learning.</em> MIT Press.</p> </li> <li> <p><strong>Kolouri, S., Pope, P. E., Martin, C. E., &amp; Rohde, G. K. (2019).</strong> Sliced Wasserstein auto-encoders. In <em>International Conference on Learning Representations (ICLR)</em>.</p> </li> <li> <p><strong>Kolouri, S., Zou, Y., &amp; Rohde, G. K. (2018).</strong> Sliced Wasserstein kernels for probability distributions. In <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 5258-5267).</p> </li> <li> <p><strong>Patrini, G., van den Berg, R., Forré, P., Carioni, M., Bhargav, S., Welling, M., Genewein, T., &amp; Roth, F. (2020).</strong> Sinkhorn autoencoders. In <em>Conference on Uncertainty in Artificial Intelligence (UAI)</em> (pp. 733-743).</p> </li> <li> <p><strong>Fan, L., Dai, W., Lee, D.-J., &amp; Liu, W. (2019).</strong> On the effectiveness of optimal transport for generative modeling. <em>arXiv preprint arXiv:1901.11373</em>.</p> </li> <li> <p><strong>Patrini, G., van den Berg, R., Forré, P., Carioni, M., Bhargav, S., Welling, M., Genewein, T., &amp; Roth, F. (2020).</strong> Optimal transport autoencoders. In <em>International Conference on Learning Representations (ICLR)</em>.</p> </li> <li> <p><strong>Rubenstein, P., Schoelkopf, B., &amp; Tolstikhin, I. (2018).</strong> On the latent space of Wasserstein auto-encoders. <em>arXiv preprint arXiv:1802.03761</em>.</p> </li> <li> <p><strong>Ghosh, P., Sajjadi, M. S. M., Vergari, A., Black, M., &amp; Schölkopf, B. (2019).</strong> From variational to deterministic autoencoders. <em>arXiv preprint arXiv:1903.12436</em>.</p> </li> <li> <p><strong>Peyré, G., &amp; Cuturi, M. (2019).</strong> Computational optimal transport. <em>Foundations and Trends in Machine Learning</em>, 11(5-6), 355-607.</p> </li> <li> <p><strong>Kodali, N., Abernethy, J., Hays, J., &amp; Kira, Z. (2017).</strong> On convergence and stability of GANs. <em>arXiv preprint arXiv:1705.07215</em>.</p> </li> <li> <p><strong>Kodali, N., Hays, J., Abernethy, J., &amp; Kira, Z. (2017).</strong> How to train your DRAGAN. <em>arXiv preprint arXiv:1705.07215</em>.</p> </li> <li> <p><strong>Adler, J., &amp; Lunz, S. (2018).</strong> Banach Wasserstein GAN. In <em>Advances in Neural Information Processing Systems 31 (NeurIPS 2018)</em> (pp. 6754-6763).</p> </li> <li> <p><strong>Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., &amp; Courville, A. (2017).</strong> Improved training of Wasserstein GANs. In <em>Advances in Neural Information Processing Systems 30 (NIPS 2017)</em> (pp. 5767-5777).</p> </li> <li> <p><strong>Petzka, H., Fischer, A., &amp; Lukovnikov, D. (2018).</strong> On the regularization of Wasserstein GANs. In <em>International Conference on Learning Representations (ICLR)</em>.</p> </li> <li> <p><strong>Patel, A. B., Nguyen, T., &amp; Baraniuk, R. G. (2020).</strong> A probabilistic framework for deep learning. In <em>Advances in Neural Information Processing Systems 29 (NIPS 2016)</em>.</p> </li> <li> <p><strong>Mescheder, L., Nowozin, S., &amp; Geiger, A. (2018).</strong> The numerics of GANs. In <em>Advances in Neural Information Processing Systems 31 (NeurIPS 2018)</em> (pp. 1825-1835).</p> </li> <li> <p><strong>Bellemare, M. G., Danihelka, I., Dabney, W., Mohamed, S., Lakshminarayanan, B., Hoyer, S., &amp; Munos, R. (2017).</strong> The Cramer distance as a solution to biased Wasserstein gradients. <em>arXiv preprint arXiv:1705.10743</em>.</p> </li> </ol>]]></content><author><name></name></author><category term="thesis-chapters-p0"/><category term="thesis"/><category term="GANs"/><category term="machine-learning"/><category term="generative-models"/><summary type="html"><![CDATA[An introduction to GANs and Wasserstein Autoencoders, covering the foundational concepts and variants]]></summary></entry><entry><title type="html">Thesis Chapter 1.1: Introduction to Generative Modeling</title><link href="https://www.siddarthasokan.com/thesis-chapters-p0/2023/05/10/thesis-chapter-1p1-introGenMod.html" rel="alternate" type="text/html" title="Thesis Chapter 1.1: Introduction to Generative Modeling"/><published>2023-05-10T00:00:00+00:00</published><updated>2023-05-10T00:00:00+00:00</updated><id>https://www.siddarthasokan.com/thesis-chapters-p0/2023/05/10/thesis-chapter-1p1-introGenMod</id><content type="html" xml:base="https://www.siddarthasokan.com/thesis-chapters-p0/2023/05/10/thesis-chapter-1p1-introGenMod.html"><![CDATA[<blockquote> <p><em>What I cannot create, I do not understand.</em><br/> — Richard P. Feynman</p> </blockquote> <h2 id="generative-models">Generative Models</h2> <p>Generative modeling is a branch of machine learning that addresses the problem of learning the distribution of given data samples, which can subsequently be used to create more such samples. Generative models have gained immense popularity in the generation of images [1], text [2], and audio [3] and learning multi-modal transformations [4]. In this thesis, we focus on image datasets, and consider generative modelling for images. Consider the dataset \(\mathcal{D} = \big\{\mathbf{x}_i~\|~\mathbf{x}_i\in\mathbb{R}^n,\,i=1,2,\ldots,N\big\}\), with samples drawn from an unknown distribution \(p_d\) (the <em>data distribution</em>). We are considering \(\mathbf{x}_i\) to be the vectorized representation of an image, with \(n = h\times w\times c\), where \((h,w,c)\) denote the height, width, and the number of channels in the image. This representation also allows one to consider multichannel images, such as color images, multispectral images, and hyperspectral images.</p> <p>Neural-network-based generative models output samples \(\tilde{\mathbf{x}}\) that are governed by a parametric distribution \(p_{\text{model}}(\tilde{\mathbf{x}};\theta)\). The goal in optimizing/training the neural network model is to learn the optimal parameters \(\theta^*\) such that the generated samples appear realistic, and \(p_{\text{model}}(\tilde{\mathbf{x}};\theta^*) = p_d\). Early neural-network-based generative models, going back to the 1980s, are energy-based models (EBMs) that define an energy functional on data samples. However, these models relied on Markov chain Monte Carlo (MCMC) for sampling [5], and could not be scaled to high-dimensional objects such as images.</p> <p>Modern approaches to generative modelling rely on the <em>Manifold Hypothesis</em> [6], which posits that although the ambient dimension of images is large \((\mathbf{x}\in\mathbb{R}^n)\), they actually reside in a much lower-dimensional manifold \((\mathbf{z}\in\mathbb{R}^d)\) with \(d\ll n\), often several orders lower. Leveraging this property, generative models have targeted nearly invertible low-dimensional <em>latent</em> representations \(\{\mathbf{z}_i\}\) of images \(\{\mathbf{x}_i\}\). Autoencoders (AEs) [7], variational autoencoders (VAEs) [8], Wasserstein autoencoders (WAEs) [9], generative adversarial networks (GANs) [10], and variants thereof, are all examples of such generative models. By contrast, advanced frameworks such as normalizing flows (NFs) [11] and diffusion models (DMs) [13] operate on the ambient dimensionality of the data and are far more challenging to train and intensive in terms of memory and compute. Our focus in this thesis is predominantly on GANs, with occasional connections to WAEs, NFs, and DMs.</p> <p>We briefly introduce the various generative models considered in this thesis.</p> <h3 id="autoencoders">Autoencoders</h3> <p>Autoencoders comprise a pair of neural networks, namely the encoder \((\text{Enc}_{\phi})\) and decoder \((\text{Dec}_{\theta})\), trained to learn a compressed (low-dimensional) representation of the data \(\mathcal{D}\). Given a sample \(\mathbf{x}_i\), the encoder learns a latent representation of the data \(\mathbf{z}_i = \text{Enc}_{\phi}(\mathbf{x}_i)\), while the decoder learns to invert the encoder transformation, \(\tilde{\mathbf{x}}_i = \text{Dec}_{\theta}(\mathbf{z}_i)\) such that \(\tilde{\mathbf{x}}_i \approx \mathbf{x}_i\). While the encoder and decoder networks are typically fully-connected multi-layer perceptron (MLP) networks, convolutional AEs (CAEs) consider encoders with convolutional layers, while the decoders consist of transposed convolution layers. While autoencoders were originally proposed as the nonlinear counterpart of principal component analysis (PCA), in the recent literature, AEs have been leveraged to implement flow-based and generative pre-training architectures.</p> <p>The standard AE optimization involves training the encoder-decoder pair to generate accurate reconstructions. Given the dataset \(\mathcal{D}\), the \(N\)-sample estimate of the AE loss is given by</p> \[\mathcal{L}^{\text{AE}}(\phi,\theta) = \frac{1}{N}\sum_{i=1,\,\mathbf{x}_i\sim\mathcal{D}}^{N}\text{dist}\left(\mathbf{x}_i,\tilde{\mathbf{x}}_i \right) = \frac{1}{N}\sum_{i=1,\,\mathbf{x}_i\sim\mathcal{D}}^{N}\text{dist}\left(\mathbf{x}_i,\text{Dec}_{\theta}\left( \text{Enc}_{\phi}\left(\mathbf{x}_i \right) \right) \right),\] <p>where in turn, \(\phi^*\) and \(\theta^*\) denote the optimal encoder and decoder parameters:</p> \[(\phi^*,\theta^*) = \arg\min_{\theta,\phi} \left\{ \mathcal{L}^{\text{AE}} \right\}.\] <p>Typical choices for the distance measure include the \(\ell_1\) or \(\ell_2\) loss:</p> \[\mathcal{L}_{\text{L}_p}^{\text{AE}}(\phi,\theta) = \frac{1}{N}\sum_{i=1,\,\mathbf{x}_i\sim\mathcal{D}}^{N}\\|\mathbf{x}_i - \text{Dec}_{\theta}\left( \text{Enc}_{\phi}\left(\mathbf{x}_i \right) \right) \\|_{\ell_p},\] <p>where \(p = 1\) or \(2\), respectively, or a weighted combination of these. Regularized autoencoders have also been considered, wherein the encoder and decoder networks are regularized to either be smooth transformations [12] or learn sparse representations [15]. Although autoencoders are generative models in the sense that the decoders learn to output realistic images given a latent representation, a major limitation is their inability to generate new/unseen images. This can be attributed to the lack of a closed-form or parametric structure to the latent-space distribution, which results in inferior reconstructions of latent-space interpolations. The variational and Wasserstein autoencoder schemes alleviate this issue, by enforcing constraints on the distribution of the latent representations.</p> <h3 id="variational-autoencoders">Variational Autoencoders</h3> <p>While AEs consider the encoder output as a <em>deterministic</em> representation of the input image, in variational autoencoders (VAEs) [8], the latent representations are assumed to be random, with the objective of modelling the joint distribution \(p_{\text{model}}(\mathbf{x},\mathbf{z})\), from which the data distribution can be computed as \(p_d(\mathbf{x}) = \int_{\mathcal{Z}} p_{\text{model}}(\mathbf{x},\mathbf{z})\,d\mathbf{z}\), where \(\mathcal{Z}\) denotes the support of the latent-space distribution. Invoking Bayes’ rule, we have \(p_{\text{model}}(\mathbf{x},\mathbf{z}) = \frac{p_{\text{model}}(\mathbf{x}\|\mathbf{z}) p_z(\mathbf{z})}{p_{\text{model}}(\mathbf{z}\|\mathbf{x})}\). The decoder can be interpreted as approximating the posterior, <em>i.e.</em>, \(\text{Dec}_{\theta}(\mathbf{z}) \sim p_{\text{model}}(\mathbf{x}\|\mathbf{z})\). As the underlying data distribution is inaccessible, the encoder models an approximation of the likelihood, \(\text{Enc}_{\phi}(\mathbf{x}) \sim q(\mathbf{z}\|\mathbf{x}) \approx p_{\text{model}}(\mathbf{z}\|\mathbf{x})\). In VAEs, one optimizes the so called evidence lower bound (ELBO), given by</p> \[\mathcal{L}_{\text{ELBO}}^{\text{VAE}} = \underbrace{\mathbb{E}_{\mathbf{z}\sim \text{Enc}_{\phi}(\mathbf{x})} \left[ \ln\left(p_{\text{model}}(\mathbf{x}\|\mathbf{z})\right)\right]}_{\text{reconstruction error}} - \underbrace{D_{\text{KL}}\left( q(\mathbf{z}\|\mathbf{x}) \,\\|\, p_z(\mathbf{z})\right)}_{\text{prior matching error}},\] <p>where \(D_{\text{KL}}\) denotes the Kullback-Leibler (KL) divergence between two distributions. The reconstruction error, as in the case of AEs, can be approximated in terms of the \(\ell_2\) or \(\ell_1\) loss. The KL divergence can be further simplified by assuming a Gaussian model \(p_z(\mathbf{z})\), with the latent sample defined by means of a <em>change-of-variable</em> formula as \(\mathbf{z}_i = \mu_{\phi}(\mathbf{x}_i)+ \sigma_{\phi}(\mathbf{x}_i)\odot\boldsymbol{\epsilon}\), where \((\mu_{\phi},\sigma_{\phi})= \text{Enc}_{\phi}(\mathbf{x}_i)\) and \(\boldsymbol{\epsilon}\) is drawn from the standard normal distribution, <em>i.e.</em>, \(\boldsymbol{\epsilon}\sim\mathcal{N}(\mathbf{0}_d,\mathbb{I}_d)\), where in turn, \(\mathbf{0}_d\) denotes the \(d\)-dimensional null vector, \(\mathbb{I}_d\) is the \(d\)-dimensional identity matrix, and \(\odot\) stands for pointwise multiplication. Although VAEs are superior to AEs in terms of enforcing structure on the latent space, generating novel unseen samples remains challenging. Further, the use of the \(\ell_2\) cost results in over-smoothing of the reconstructed images, owing to the implicit Gaussian prior that this choice of loss introduces [16]. Generative adversarial networks (GANs) [10] were proposed as an alternative to VAEs, to alleviate its limitations.</p> <h3 id="navigation">Navigation</h3> <ul> <li><strong>Next:</strong> <a href="/thesis-chapters/2023/05/10/thesis-chapter-1p2-IntroGANs.html">Chapter 1.2: Generative Adversarial Networks</a></li> <li><strong>Back to:</strong> <a href="/projects/1_thesis">Thesis Project</a></li> </ul> <hr/> <h2 id="references">References</h2> <ol> <li> <p><strong>Kang, M., Zhu, J.-Y., Zhang, R., Park, J., Shechtman, E., Paris, S., &amp; Park, T. (2023).</strong> Scaling up GANs for text-to-image synthesis. In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>.</p> </li> <li> <p><strong>Guo, J., Lu, S., Cai, H., Zhang, W., Yu, Y., &amp; Wang, J. (2018).</strong> Long text generation via adversarial training with leaked information. <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>, 32.</p> </li> <li> <p><strong>Engel, J., Agrawal, K. K., Chen, S., Gulrajani, I., Donahue, C., &amp; Roberts, A. (2019).</strong> GANSynth: Adversarial neural audio synthesis. In <em>International Conference on Learning Representations (ICLR)</em>.</p> </li> <li> <p><strong>Yu, J., Xu, Y., Koh, J. Y., Luong, T., Baid, G., Wang, Z., Vasudevan, V., Ku, A., Yang, Y., Ayan, B. K., Hutchinson, B., Han, W., Parekh, A., Li, X., Zhang, H., Baldridge, J., &amp; Wu, Y. (2022).</strong> Scaling autoregressive models for content-rich text-to-image generation. <em>arXiv preprint arXiv:2206.10789</em>.</p> </li> <li> <p><strong>Kalos, M. H., &amp; Whitlock, P. A. (1986).</strong> <em>Monte Carlo Methods.</em> Wiley Publications.</p> </li> <li> <p><strong>Carlsson, G., Ishkhanov, T., de Silva, V., &amp; Zomorodian, A. (2008).</strong> On the local behavior of spaces of natural images. <em>International Journal of Computer Vision</em>, 76(1), 1-12.</p> </li> <li> <p><strong>Hinton, G. E., &amp; Zemel, R. S. (1994).</strong> Autoencoders, minimum description length and Helmholtz free energy. In <em>Advances in Neural Information Processing Systems 6 (NIPS 1993)</em> (pp. 3-10).</p> </li> <li> <p><strong>Kingma, D. P., &amp; Welling, M. (2013).</strong> Auto-encoding variational Bayes. <em>arXiv preprint arXiv:1312.6114</em>.</p> </li> <li> <p><strong>Tolstikhin, I., Bousquet, O., Gelly, S., &amp; Schoelkopf, B. (2018).</strong> Wasserstein auto-encoders. In <em>International Conference on Learning Representations (ICLR)</em>.</p> </li> <li> <p><strong>Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., &amp; Bengio, Y. (2014).</strong> Generative adversarial nets. In <em>Advances in Neural Information Processing Systems 27 (NIPS 2014)</em> (pp. 2672-2680).</p> </li> <li> <p><strong>Rezende, D., &amp; Mohamed, S. (2015).</strong> Variational inference with normalizing flows. In <em>International Conference on Machine Learning (ICML)</em> (pp. 1530-1538).</p> </li> <li> <p><strong>Liang, K., Chang, H., Cui, Z., Shan, S., &amp; Chen, X. (2015).</strong> Representation learning with smooth autoencoder. In <em>12th Asian Conference on Computer Vision</em> (pp. 72-86).</p> </li> <li> <p><strong>Ho, J., Jain, A., &amp; Abbeel, P. (2020).</strong> Denoising diffusion probabilistic models. In <em>Advances in Neural Information Processing Systems 33 (NeurIPS 2020)</em> (pp. 6840-6851).</p> </li> <li> <p><strong>Rifai, S., Mesnil, G., Vincent, P., Muller, X., Bengio, Y., Dauphin, Y., &amp; Glorot, X. (2011).</strong> Higher order contractive auto-encoder. In <em>Joint European Conference on Machine Learning and Knowledge Discovery in Databases</em> (pp. 645-660). Springer.</p> </li> <li> <p><strong>Ng, A. (2011).</strong> Sparse autoencoder. <em>CS294A Lecture Notes</em>, 72, 1-19.</p> </li> <li> <p><strong>Figueiredo, M. (2001).</strong> Adaptive sparseness using Jeffreys prior. In <em>Advances in Neural Information Processing Systems</em> 2001, 14.</p> </li> </ol>]]></content><author><name></name></author><category term="thesis-chapters-p0"/><category term="thesis"/><category term="GANs"/><category term="machine-learning"/><category term="generative-models"/><summary type="html"><![CDATA[An introduction to generative modeling, covering autoencoders, VAEs, GANs, flows, and diffusion models]]></summary></entry></feed>