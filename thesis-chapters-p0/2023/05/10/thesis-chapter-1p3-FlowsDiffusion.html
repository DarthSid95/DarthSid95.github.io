<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Thesis Chapter 1.3: Flow-based Models and Diffusion | Siddarth Asokan </title> <meta name="author" content="Siddarth Asokan"> <meta name="description" content="An introduction to normalizing flows and score-based diffusion models"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/s-solid.svg?67c229b8af66c0d7a1d675d8397f5760"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://www.siddarthasokan.com/thesis-chapters-p0/2023/05/10/thesis-chapter-1p3-FlowsDiffusion.html"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Siddarth</span> Asokan </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/bio/">Bio </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">News </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/doggo/"><i class="fa-solid fa-dog"></i> </a> </li> <li class="nav-item "> <a class="nav-link" href="/photos/"><i class="fa-solid fa-images"></i> </a> </li> <li class="nav-item "> <a class="nav-link" href="/compare/"><i class="fa-solid fa-brain"></i> AI </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Others </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/bio/">Bio</a> <a class="dropdown-item " href="/teaching/">Teaching</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/">Projects</a> <a class="dropdown-item " href="/repositories/">Repositories</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Thesis Chapter 1.3: Flow-based Models and Diffusion</h1> <p class="post-meta"> May 10, 2023 </p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2023   ·   <i class="fa-solid fa-hashtag fa-sm"></i> thesis   <i class="fa-solid fa-hashtag fa-sm"></i> flows   <i class="fa-solid fa-hashtag fa-sm"></i> diffusion   <i class="fa-solid fa-hashtag fa-sm"></i> machine-learning   <i class="fa-solid fa-hashtag fa-sm"></i> generative-models     ·   <i class="fa-solid fa-tag fa-sm"></i> thesis-chapters-p0   </p> </header> <article class="post-content"> <div id="table-of-contents"> </div> <hr> <div id="markdown-content"> <blockquote> <p><em>What I cannot create, I do not understand.</em><br> — Richard P. Feynman</p> </blockquote> <h2 id="flow-based-models">Flow-based Models</h2> <p>Flow-based models such as normalizing flows (NFs) [1] leverage the <em>change-of-variables</em> formula to learn a transformation from a parametric prior distribution to the target. Unlike in GANs and AEs, in NF models, the input to model \(\mathbf{z}\) is assumed to be of the same dimensionality as the output, <em>i.e.</em>, \(\mathbf{z}\in\mathbb{R}^{n}\). Normalizing flows model a forward process as the gradual <em>corruption</em> of a target distribution \(p_d\) to the noise distribution \(p_z\). The forward process is modeled as a series of <em>invertible</em> transformations \(f_i\), such that the composite function yields the desired <em>push-forward</em> distribution:</p> \[\mathbf{z} = \left( f_N \circ f_{N-1} \circ \cdots \circ f_2 \circ f_1\right) (\mathbf{x}) = f(\mathbf{x});~\mathbf{x}\sim p_d.\] <p>Then, by the <em>change-of-variables</em> formula, we have</p> \[p_z\left(\mathbf{z}\right) = p_d\left(f^{-1}(\mathbf{z})\right) \left\vert \mathrm{det}\, \mathbf{J}_{f^{-1}}(\mathbf{z})\right\vert = p_d\left(f^{-1}(\mathbf{z})\right) \left\vert \mathrm{det}\, \mathbf{J}_{f}\left(f^{-1}(\mathbf{z})\right)\right\vert^{-1},\] <p>where \(\left\vert \mathrm{det}\, \mathbf{J}_{f}(\cdot)\right\vert\) denotes the determinant of the Jacobian of \(f\). Consider a vector \(\mathbf{x} = [x_1, x_2,\,\ldots\,,x_n]^{\mathsf{T}} \in \mathbb{R}^n\) and the function \(f:\mathbb{R}^n \rightarrow \mathbb{R}^n\), <em>i.e.</em>, \(f(\mathbf{x}) = [f^1(\mathbf{x}), f^2(\mathbf{x}),\ldots,f^n(\mathbf{x})]\). The notation \(\nabla_{\mathbf{x}} f(\mathbf{x})\) represents the gradient matrix associated with the generator, with entries consisting of the partial derivatives of the entries of \(f\) with respect to the entries of \(\mathbf{x}\):</p> \[\nabla_{\mathbf{x}} f(\mathbf{x}) = \left[\begin{matrix} \frac{\partial f^1}{\partial x_1} &amp; \frac{\partial f^2}{\partial x_1} &amp; \ldots &amp; \ldots &amp;\frac{\partial f^n}{\partial x_1} \\[3pt] \frac{\partial f^1}{\partial x_2} &amp; \frac{\partial f^2}{\partial x_2} &amp; \ldots &amp; \ldots &amp;\frac{\partial f^n}{\partial x_2} \\[3pt] \vdots&amp;\vdots&amp;\ddots&amp;\ddots&amp;\vdots\\ \frac{\partial f^1}{\partial x_n} &amp; \frac{\partial f^2}{\partial x_n} &amp; \ldots &amp; \ldots &amp;\frac{\partial f^n}{\partial x_n} \end{matrix} \right].\] <p>The Jacobian \(\mathbf{J}\) can be thought of as <em>measuring</em> the transformation that the function imposes locally at the point of evaluation, and is defined to be the transpose of the gradient, <em>i.e.,</em> \(\nabla_{\mathbf{z}}f(\mathbf{z}) = \mathbf{J}_f^{\mathsf{T}}(\mathbf{z})\). The generative model is then defined as the <em>reverse</em> process, given by:</p> \[p_d(\mathbf{x}) = p_z\left(f^{-1}(\mathbf{x})\right) \left\vert \mathrm{det}\, \mathbf{J}_{f}(\mathbf{x})\right\vert = p_z\left(f^{-1}(\mathbf{x})\right) \prod_{i=1}^{N} \left\vert \mathrm{det}\, \mathbf{J}_{f_i}(\mathbf{z}_{i-1})\right\vert,\] <p>where \(\mathbf{z}_i = f_i(\mathbf{z}_{i-1})\) with \(\mathbf{z}_0 = \mathbf{z}\) and \(\mathbf{z}_N = \mathbf{x}\). An in-depth analysis of normalizing flows is presented by recent comprehensive surveys [2]. The \(f_i\) network architecture is constrained to facilitate easy computation of the Jacobian. Recent approaches design flows based on autoregressive models [3,4,5,6,7], or architectures motivated by the Sobolev GAN loss [9,10]. Flows have also been explored in the context of improving the quality of the noise input in GANs [8,11,12].</p> <h2 id="score-based-diffusion-models">Score-based Diffusion Models</h2> <p>Diffusion models are a recent class of generative models that implement discretized Langevin flow [13], and can be viewed as a random walk in a high-dimensional space. The random walks correspond to a Markov chain, whose stationary distribution converges to the desired distribution. As in the case of flow-based approaches, a forward process is assumed, wherein the data is <em>corrupted</em> with noise, while the generative model is an approximation of the reverse process. Langevin Monte Carlo (LMC) is the canonical algorithm for sampling from a given distribution in this framework, where we have access to the <em>score function</em>, \(\nabla_{\mathbf{x}}\ln p_d(\mathbf{x})\), which is the gradient of the logarithm of the target distribution. LMC is an instance of Markov chain Monte Carlo (MCMC) [16], and is a discretization of the Langevin diffusion stochastic differential equation given by</p> \[d\mathbf{x}(t) = -\nabla_{\mathbf{x}}f(\mathbf{x}(t))\,dt + \sqrt{2}\,d\mathbf{B}(t),\] <p>where \(\mathbf{B}(t)\) is the standard Brownian motion. The associated discretized update is given by</p> \[\mathbf{x}_{t+1} = \mathbf{x}_{t} - \epsilon \left. \nabla_{\mathbf{x}}f(\mathbf{x}) \right|_{\mathbf{x}=\mathbf{x}_t} + \sqrt{2\epsilon}\,\mathbf{z}_t,\] <p>where \(\mathbf{z}_{t}\) is an instance of the noise distribution drawn at time instant \(t\) and is independent of the sample \(\mathbf{x}_{t}\) generated at the corresponding time instant. As in normalizing flows, we have \(\mathbf{z}\in\mathbb{R}^{n}\). The evolution is initialized with parametric noise, <em>i.e.</em>, \(\mathbf{x}_0 \sim p_z\). The choice of the mapping function \(f\) gives rise to various diffusion models. For example, the popular family of score-based models [14,15] consider \(f(\mathbf{x}) = \nabla_{\mathbf{x}}\ln p_d(\mathbf{x})\), or in inverse-heat diffusion models [24], we have a frequency-domain transformation corresponding to Gaussian deblurring. In this thesis, we primarily focus on score-based diffusion, and their relation to GAN optimization.</p> <p>Existing score-based approaches train a neural network \(s_{\theta}(\mathbf{x})\) to approximate the score, by means of a score-matching loss [17], originally considered in the context of independent component analysis:</p> \[\mathcal{L}(\theta)=\frac{1}{2} \,\mathbb{E}_\sim p_d} \left[ \left\Vert s_{\theta}(\mathbf{x}) - \nabla_{\mathbf{x}}\ln p_d(\mathbf{x}) \right\Vert \right\Vert_2^2 \right].\] <p>The output of the trained network is used to generate samples through the annealed Langevin dynamics in noise-conditioned score networks (NCSN) [14]. However, a major limitation is that the predicted score is <em>weak</em> in regions far away from the target distribution, which is typically the case at the start of the Markov chain, around \(\mathbf{x}_0\sim p_z\). Various approaches such as noise scaling [14], sliced SM [18], and denoising SM [13,15] have been proposed to improve the strength of the gradients. Works considering improved discretization of the underlying differential equation [15,20,21,22] have been developed to accelerate the sampling process. Recently, denoising diffusion GANs (DDGANs) [23] were introduced, wherein a GAN is trained to model the diffusion process, with the generator and discriminator networks conditioned on the time index.</p> <h3 id="navigation">Navigation</h3> <ul> <li> <strong>Previous:</strong> <a href="/thesis-chapters/2023/05/10/thesis-chapter-1p2-IntroGANs.html">Chapter 1.2: Generative Adversarial Networks</a> </li> <li> <strong>Next:</strong> <a href="/blog/2023/thesis-chapter-1p4-VariationalCalculus/">Chapter 1.4: An Introduction to Variational Calculus</a> </li> <li> <strong>Back to:</strong> <a href="/projects/1_thesis/">Thesis Project</a> </li> </ul> <hr> <h2 id="references">References</h2> <ol> <li> <p><strong>Rezende, D., &amp; Mohamed, S. (2015).</strong> Variational inference with normalizing flows. In <em>International Conference on Machine Learning (ICML)</em> (pp. 1530-1538).</p> </li> <li> <p><strong>Papamakarios, G., Nalisnick, E., Rezende, D. J., Mohamed, S., &amp; Lakshminarayanan, B. (2021).</strong> Normalizing flows for probabilistic modeling and inference. <em>Journal of Machine Learning Research</em>, 22(57), 1-64.</p> </li> <li> <p><strong>Dinh, L., Krueger, D., &amp; Bengio, Y. (2015).</strong> NICE: Non-linear independent components estimation. In <em>International Conference on Learning Representations (ICLR) Workshop</em>.</p> </li> <li> <p><strong>Dinh, L., Sohl-Dickstein, J., &amp; Bengio, S. (2017).</strong> Density estimation using Real NVP. In <em>International Conference on Learning Representations (ICLR)</em>.</p> </li> <li> <p><strong>Kingma, D. P., &amp; Dhariwal, P. (2018).</strong> Glow: Generative flow using invertible 1×1 convolutions. In <em>Advances in Neural Information Processing Systems 31 (NeurIPS 2018)</em> (pp. 10215-10224).</p> </li> <li> <p><strong>Kingma, D. P., Salimans, T., Jozefowicz, R., Chen, X., Sutskever, I., &amp; Welling, M. (2016).</strong> Improved variational inference with inverse autoregressive flow. In <em>Advances in Neural Information Processing Systems 29 (NIPS 2016)</em> (pp. 4743-4751).</p> </li> <li> <p><strong>Papamakarios, G., Pavlakou, T., &amp; Murray, I. (2017).</strong> Masked autoregressive flow for density estimation. In <em>Advances in Neural Information Processing Systems 30 (NIPS 2017)</em> (pp. 2338-2347).</p> </li> <li> <p><strong>Chen, X., Kingma, D. P., Salimans, T., Duan, Y., Dhariwal, P., Schulman, J., Sutskever, I., &amp; Abbeel, P. (2018).</strong> Variational lossy autoencoder. In <em>International Conference on Learning Representations (ICLR)</em>.</p> </li> <li> <p><strong>Mroueh, Y., Sercu, T., &amp; Goel, V. (2019).</strong> Sobolev descent. In <em>International Conference on Artificial Intelligence and Statistics (AISTATS)</em> (pp. 2976-2984).</p> </li> <li> <p><strong>Gong, C., Wang, D., &amp; Liu, Q. (2020).</strong> Unbiased Sobolev descent. In <em>International Conference on Machine Learning (ICML)</em> (pp. 3558-3567).</p> </li> <li> <p><strong>Kumar, M., Weissenborn, D., &amp; Kalchbrenner, N. (2021).</strong> Regularizing normalizing flows with Kallenberg-Leibler divergence. In <em>Conference on Uncertainty in Artificial Intelligence (UAI)</em>.</p> </li> <li> <p><strong>Dinh, L., Sohl-Dickstein, J., Pascanu, R., &amp; Larochelle, H. (2021).</strong> A RAD approach to deep mixture models. In <em>International Conference on Learning Representations (ICLR)</em>.</p> </li> <li> <p><strong>Ho, J., Jain, A., &amp; Abbeel, P. (2020).</strong> Denoising diffusion probabilistic models. In <em>Advances in Neural Information Processing Systems 33 (NeurIPS 2020)</em> (pp. 6840-6851).</p> </li> <li> <p><strong>Song, Y., &amp; Ermon, S. (2019).</strong> Generative modeling by estimating gradients of the data distribution. In <em>Advances in Neural Information Processing Systems 32 (NeurIPS 2019)</em> (pp. 11895-11907).</p> </li> <li> <p><strong>Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., &amp; Poole, B. (2021).</strong> Score-based generative modeling through stochastic differential equations. In <em>International Conference on Learning Representations (ICLR)</em>.</p> </li> <li> <p><strong>Sinclair, A. (1992).</strong> Improved bounds for mixing rates of Markov chains and multicommodity flow. <em>Combinatorics, Probability and Computing</em>, 1(4), 351-370.</p> </li> <li> <p><strong>Hyvärinen, A. (2005).</strong> Estimation of non-normalized statistical models by score matching. <em>Journal of Machine Learning Research</em>, 6, 695-709.</p> </li> <li> <p><strong>Song, Y., &amp; Ermon, S. (2020).</strong> Improved techniques for training score-based generative models. In <em>Advances in Neural Information Processing Systems 33 (NeurIPS 2020)</em> (pp. 12438-12448).</p> </li> <li> <p><strong>Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., &amp; Poole, B. (2021).</strong> Score-based generative modeling through stochastic differential equations. In <em>International Conference on Learning Representations (ICLR)</em>.</p> </li> <li> <p><strong>Song, Y., Durkan, C., Murray, I., &amp; Ermon, S. (2020).</strong> Maximum likelihood training of score-based diffusion models. In <em>Advances in Neural Information Processing Systems 33 (NeurIPS 2020)</em> (pp. 1415-1428).</p> </li> <li> <p><strong>Jolicoeur-Martineau, A., Li, K., Piché-Taillefer, R., Kachman, T., &amp; Mitliagkas, I. (2021).</strong> Gotta go fast when generating data with score-based models. <em>arXiv preprint arXiv:2105.14080</em>.</p> </li> <li> <p><strong>Karras, T., Aittala, M., Aila, T., &amp; Laine, S. (2022).</strong> Elucidating the design space of diffusion-based generative models. In <em>Advances in Neural Information Processing Systems 35 (NeurIPS 2022)</em> (pp. 26565-26577).</p> </li> <li> <p><strong>Xiao, Z., Kreis, K., &amp; Vahdat, A. (2022).</strong> Tackling the generative learning trilemma with denoising diffusion GANs. In <em>International Conference on Learning Representations (ICLR)</em>.</p> </li> <li> <p><strong>Bansal, A., Borgnia, E., Chu, H.-M., Li, J., Kazemi, H., Huang, F., Goldblum, M., Geiping, J., &amp; Goldstein, T. (2023).</strong> Cold diffusion: Inverting arbitrary image transforms without noise. In <em>Advances in Neural Information Processing Systems 36 (NeurIPS 2023)</em>.</p> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Siddarth Asokan. Powered by my desire to have a website for myself, and by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: October 17, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-FNZLVV2TEH"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-FNZLVV2TEH");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>